@misc{devlinBERTPretrainingDeep2018,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1810.04805},
  urldate = {2025-10-30},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@misc{dosovitskiyImageWorth16x162020,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2010.11929},
  urldate = {2025-10-30},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@book{firth1957synopsis,
  title = {A Synopsis of Linguistic Theory, 1930-1955},
  author = {Firth, J.R.},
  year = {1957}
}

@article{harrisDistributionalStructure1954,
  title = {Distributional {{Structure}}},
  author = {Harris, Zellig S.},
  year = {1954},
  month = aug,
  journal = {\emph{WORD}},
  volume = {10},
  number = {2-3},
  pages = {146--162},
  issn = {0043-7956, 2373-5112},
  doi = {10.1080/00437956.1954.11659520},
  urldate = {2025-10-30},
  langid = {english}
}

@book{jm3,
  title = {Speech and Language Processing: {{An}} Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2025},
  edition = {3rd}
}

@article{landauerSolutionPlatosProblem1997,
  title = {A Solution to {{Plato}}'s Problem: {{The}} Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.},
  shorttitle = {A Solution to {{Plato}}'s Problem},
  author = {Landauer, Thomas K. and Dumais, Susan T.},
  year = {1997},
  month = apr,
  journal = {Psychological Review},
  volume = {104},
  number = {2},
  pages = {211--240},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.104.2.211},
  urldate = {2025-10-30},
  langid = {english}
}

@inproceedings{luongEffectiveApproachesAttentionbased2015,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  pages = {1412--1421},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1166},
  urldate = {2025-10-30},
  langid = {english},
  file = {/home/gcortal/Zotero/storage/EEBIJ7F9/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Machine Translation.pdf}
}

@inproceedings{NEURIPS2020_1457c0d6,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.}
}

@inproceedings{NIPS2013_9aa42b31,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  editor = {Burges, C.J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K.Q.},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.}
}

@inproceedings{NIPS2015_8fb21ee7,
  title = {End-to-End Memory Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sukhbaatar, Sainbayar and {szlam}, arthur and Weston, Jason and Fergus, Rob},
  editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.}
}

@book{osgoodMeasurementMeaning1957,
  title = {The {{Measurement}} of {{Meaning}}},
  author = {Osgood, Charles Egerton},
  editor = {Hildum, Donald C.},
  year = {1957},
  publisher = {University of Illinois Press},
  address = {Urbana,},
  file = {/home/gcortal/Zotero/storage/L2R57E6I/OSGTMO.html}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.02155},
  urldate = {2025-10-30},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/gcortal/Zotero/storage/6SGRE6PR/Ouyang et al. - 2022 - Training language models to follow instructions with human feedback.pdf;/home/gcortal/Zotero/storage/BRHMP7B3/2203.html}
}

@article{radfordImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {/home/gcortal/Zotero/storage/HK4FC3A4/Radford et al. - Improving Language Understanding by Generative Pre-Training.pdf}
}

@article{saltonTermweightingApproachesAutomatic1988,
  title = {Term-Weighting Approaches in Automatic Text Retrieval},
  author = {Salton, Gerard and Buckley, Christopher},
  year = {1988},
  month = jan,
  journal = {Information Processing \& Management},
  volume = {24},
  number = {5},
  pages = {513--523},
  issn = {03064573},
  doi = {10.1016/0306-4573(88)90021-0},
  urldate = {2025-10-30},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/gcortal/Zotero/storage/G73PTRGA/Salton et Buckley - 1988 - Term-weighting approaches in automatic text retrieval.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017b,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and ukasz Kaiser, {\L} and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-10-30},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {/home/gcortal/Zotero/storage/EZAAM8L9/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@book{wittgensteinPhilosophicalInvestigations1953,
  title = {Philosophical {{Investigations}}},
  author = {Wittgenstein, Ludwig},
  editor = {Anscombe, G. E. M.},
  year = {1953},
  publisher = {Wiley-Blackwell},
  address = {New York, NY, USA},
  file = {/home/gcortal/Zotero/storage/M3EYDP8Z/WITPI-4.html}
}

@misc{greschner2025categoricalemotionsappraisals,
  title = {Categorical Emotions or Appraisals - Which Emotion
                  Model Explains Argument Convincingness Better?},
  author = {Lynn Greschner and Meike Bauer and Sabine Weber and
                  Roman Klinger},
  year = {2025},
  eprint = {2511.07162},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/2511.07162},
  internaltype = {preprint}
}

@article{Troiano2023,
  author = {Enrica Troiano and Laura Oberl\"ander and Roman
                  Klinger},
  title = {Dimensional Modeling of Emotions in Text with
                  Appraisal Theories: Corpus Creation, Annotation
                  Reliability, and Prediction},
  journal = {Computational Linguistics},
  number = 1,
  volume = 49,
  month = mar,
  year = 2023,
  address = {Cambridge, MA},
  publisher = {MIT Press},
  abstract = {The most prominent tasks in emotion analysis are to
                  assign emotions to texts and to understand how
                  emotions manifest in language. An observation for
                  NLP is that emotions can be communicated implicitly
                  by referring to events, appealing to an empathetic,
                  intersubjective understanding of events, even
                  without explicitly mentioning an emotion name. In
                  psychology, the class of emotion theories known as
                  appraisal theories aims at explaining the link
                  between events and emotions. Appraisals can be
                  formalized as variables that measure a cognitive
                  evaluation by people living through an event that
                  they consider relevant. They include the assessment
                  if an event is novel, if the person considers
                  themselves to be responsible, if it is in line with
                  the own goals, and many others. Such appraisals
                  explain which emotions are developed based on an
                  event, e.g., that a novel situation can induce
                  surprise or one with uncertain consequences could
                  evoke fear. We analyze the suitability of appraisal
                  theories for emotion analysis in text with the goal
                  of understanding if appraisal concepts can reliably
                  be reconstructed by annotators, if they can be
                  predicted by text classifiers, and if appraisal
                  concepts help to identify emotion categories. To
                  achieve that, we compile a corpus by asking people
                  to textually describe events that triggered
                  particular emotions and to disclose their
                  appraisals. Then, we ask readers to reconstruct
                  emotions and appraisals from the text. This setup
                  allows us to measure if emotions and appraisals can
                  be recovered purely from text and provides a human
                  baseline. Our comparison of text classification
                  methods to human annotators shows that both can
                  reliably detect emotions and appraisals with similar
                  performance. Therefore, appraisals constitute an
                  alternative computational emotion analysis paradigm
                  and further improve the categorization of emotions
                  in text with joint models.},
  doi = {10.1162/coli_a_00461},
  url = {https://doi.org/10.1162/coli_a_00461},
  internaltype = {journal},
  archiveprefix = {arXiv},
  eprint = {2206.05238}
}

@inproceedings{pachet,
author = {Barbieri, Gabriele and Pachet, Fran\c{c}ois and Roy, Pierre
and Esposti, Mirko Degli},
title = {Markov constraints for generating lyrics with style},
year = {2012},
isbn = {9781614990970},
publisher = {IOS Press},
address = {NLD},
abstract = {We address the issue of generating texts in the style of
an existing author, that also satisfy structural constraints imposed
by the genre of the text. We focus on song lyrics, for which
structural constraints are well-defined: rhyme and meter. Although
Markov processes are known to be suitable for representing style, they
are difficult to control in order to satisfy non-local properties,
such as structural constraints, that require long distance modeling.
We show that the framework of Constrained Markov Processes allows us
to precisely generate texts that are consistent with a corpus, while
being controllable in terms of rhymes and meter, a result that no
other technique, to our knowledge, could achieve to date. Controlled
Markov processes consist in reformulating Markov processes in the
context of constraint satisfaction. We describe how to represent
stylistic and structural properties in terms of constraints in this
framework and we provide an evaluation of our method by comparing it
to both pure Markov and pure constraint-based approaches. We show how
this approach can be used for the semi-automatic generation of lyrics
in the style of a popular author that has the same structure as an
existing song.},
booktitle = {Proceedings of the 20th European Conference on Artificial
Intelligence},
pages = {115–120},
numpages = {6},
location = {Montpellier, France},
series = {ECAI'12}
}

@misc{zhang2025preferencelearningunlocksllms,
      title={Preference Learning Unlocks LLMs' Psycho-Counseling Skills}, 
      author={Mian Zhang and Shaun M. Eack and Zhiyu Zoey Chen},
      year={2025},
      eprint={2502.19731},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.19731}, 
}

@misc{sharma2025understandingsycophancylanguagemodels,
      title={Towards Understanding Sycophancy in Language Models}, 
      author={Mrinank Sharma and Meg Tong and Tomasz Korbak and David Duvenaud and Amanda Askell and Samuel R. Bowman and Newton Cheng and Esin Durmus and Zac Hatfield-Dodds and Scott R. Johnston and Shauna Kravec and Timothy Maxwell and Sam McCandlish and Kamal Ndousse and Oliver Rausch and Nicholas Schiefer and Da Yan and Miranda Zhang and Ethan Perez},
      year={2025},
      eprint={2310.13548},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.13548}, 
}

@inproceedings{tammewar-etal-2020-annotation,
    title = "Annotation of Emotion Carriers in Personal Narratives",
    author = "Tammewar, Aniruddha  and
      Cervone, Alessandra  and
      Messner, Eva-Maria  and
      Riccardi, Giuseppe",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.189/",
    pages = "1517--1525",
    language = "eng",
    ISBN = "979-10-95546-34-4",
    abstract = "We are interested in the problem of understanding personal narratives (PN) - spoken or written - recollections of facts, events, and thoughts. For PNs, we define emotion carriers as the speech or text segments that best explain the emotional state of the narrator. Such segments may span from single to multiple words, containing for example verb or noun phrases. Advanced automatic understanding of PNs requires not only the prediction of the narrator{'}s emotional state but also to identify which events (e.g. the loss of a relative or the visit of grandpa) or people (e.g. the old group of high school mates) carry the emotion manifested during the personal recollection. This work proposes and evaluates an annotation model for identifying emotion carriers in spoken personal narratives. Compared to other text genres such as news and microblogs, spoken PNs are particularly challenging because a narrative is usually unstructured, involving multiple sub-events and characters as well as thoughts and associated emotions perceived by the narrator. In this work, we experiment with annotating emotion carriers in speech transcriptions from the Ulm State-of-Mind in Speech (USoMS) corpus, a dataset of PNs in German. We believe this resource could be used for experiments in the automatic extraction of emotion carriers from PN, a task that could provide further advancements in narrative understanding."
}

@inproceedings{rathner18b_interspeech,
  title     = {State of Mind: Classification through Self-reported Affect and Word Use in Speech.},
  author    = {Eva-Maria Rathner and Yannik Terhorst and Nicholas Cummins and Björn Schuller and Harald Baumeister},
  year      = {2018},
  booktitle = {Interspeech 2018},
  pages     = {267--271},
  doi       = {10.21437/Interspeech.2018-2043},
  issn      = {2958-1796},
}