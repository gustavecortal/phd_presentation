@STRING{ACL = "Association for Computational Linguistics"}
@STRING{ACL:2020:MAIN = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"}
@STRING{ACL:2022:LONG = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}
@STRING{COLING:2018:1 = "Proceedings of the 27th International Conference on Computational Linguistics"}
@STRING{COLING:2020:MAIN = "Proceedings of the 28th International Conference on Computational Linguistics"}
@STRING{EACL:2017:2 = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers"}
@STRING{EMNLP:2021:MAIN = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"}
@STRING{KONVENS:2021:1 = "Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021)"}
@STRING{LATECHCLFL:2023:1 = "Proceedings of the 7th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature"}
@STRING{LREC:2024:MAIN = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"}
@STRING{NAACL:2019:1 = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"}
@STRING{WS:2010:2 = "Proceedings of the {NAACL} {HLT} 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text"}

@INPROCEEDINGS{alhafniPersonalizedTextGeneration2024b,
  ABSTRACT = {As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized. However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes. We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance. We make our code, data, models, and benchmarks publicly available.},
  AUTHOR = {Alhafni, Bashar and Kulkarni, Vivek and Kumar, Dhruv and Raheja, Vipul},
  EDITOR = {Deshpande, Ameet and Hwang, EunJeong and Murahari, Vishvak and Park, Joon Sung and Yang, Diyi and Sabharwal, Ashish and Narasimhan, Karthik and Kalyan, Ashwin},
  LOCATION = {St. Julians, Malta},
  PUBLISHER = ACL,
  BOOKTITLE = {Proceedings of the 1st Workshop on Personalization of Generative {{AI}} Systems ({{PERSONALIZE}} 2024)},
  DATE = {2024},
  PAGES = {88--101},
  TITLE = {Personalized Text Generation with Fine-Grained Linguistic Control},
}

@ARTICLE{ALTSZYLER2017178,
  ABSTRACT = {Computer-based dreams content analysis relies on word frequencies within predefined categories in order to identify different elements in text. As a complementary approach, we explored the capabilities and limitations of word-embedding techniques to identify word usage patterns among dream reports. These tools allow us to quantify words associations in text and to identify the meaning of target words. Word-embeddings have been extensively studied in large datasets, but only a few studies analyze semantic representations in small corpora. To fill this gap, we compared Skip-gram and Latent Semantic Analysis ({LSA}) capabilities to extract semantic associations from dream reports. {LSA} showed better performance than Skip-gram in small size corpora in two tests. Furthermore, {LSA} captured relevant word associations in dream collection, even in cases with low-frequency words or small numbers of dreams. Word associations in dreams reports can thus be quantified by {LSA}, which opens new avenues for dream interpretation and decoding.},
  AUTHOR = {Altszyler, Edgar and Ribeiro, Sidarta and Sigman, Mariano and Fernández Slezak, Diego},
  URL = {https://www.sciencedirect.com/science/article/pii/S1053810017301034},
  DATE = {2017},
  DOI = {https://doi.org/10.1016/j.concog.2017.09.004},
  ISSN = {1053-8100},
  JOURNALTITLE = {Consciousness and Cognition},
  KEYWORDS = {Dream content analysis,Latent Semantic Analysis,Word2vec},
  PAGES = {178--187},
  TITLE = {The interpretation of dream meaning: Resolving ambiguity using Latent Semantic Analysis in a small corpus of text},
  VOLUME = {56},
}

@BOOK{americanpsychiatricassociationDiagnosticStatisticalManual2013,
  AUTHOR = {{American Psychiatric Association}},
  PUBLISHER = {American Psychiatric Association},
  DATE = {2013},
  ISBN = {978-0-89042-555-8 978-0-89042-557-2},
  TITLE = {Diagnostic and Statistical Manual of Mental Disorders},
}

@ARTICLE{apperly_humans_2009,
  AUTHOR = {Apperly, Ian A. and Butterfill, Stephen A.},
  URL = {https://psycnet.apa.org/record/2009-18254-013},
  DATE = {2009},
  FILE = {Available Version (via Google Scholar):/Users/bonard/switchdrive2/Zotero/storage/JSYA7VUK/Apperly et Butterfill - 2009 - Do humans have two systems to track beliefs and be.pdf:application/pdf},
  JOURNALTITLE = {Psychological review},
  NOTE = {Publisher: American Psychological Association},
  NUMBER = {4},
  PAGES = {953},
  TITLE = {Do humans have two systems to track beliefs and belief-like states?},
  URLDATE = {2023-12-08},
  VOLUME = {116},
}

@ARTICLE{argamonAutomaticallyProfilingAuthor2009,
  AUTHOR = {Argamon, Shlomo and Koppel, Moshe and Pennebaker, James W. and Schler, Jonathan},
  DATE = {2009},
  FILE = {C:\Users\Gustave\Zotero\storage\4JISW7GV\Argamon et al. - 2009 - Automatically profiling the author of an anonymous.pdf},
  ISSN = {0001-0782},
  JOURNALTITLE = {Communications of The Acm},
  NUMBER = {2},
  PAGES = {119--123},
  SHORTJOURNAL = {Commun. ACM},
  TITLE = {Automatically Profiling the Author of an Anonymous Text},
  VOLUME = {52},
}

@BOOK{arnold_emotion_1960,
  AUTHOR = {Arnold, Magda B.},
  LOCATION = {New York},
  PUBLISHER = {Columbia University Press},
  DATE = {1960},
  TITLE = {Emotion and {Personality}},
}

@ARTICLE{aru_mind_2023,
  ABSTRACT = {Theory of Mind (ToM) is an essential ability of humans to infer the mental states of others. Here we provide a coherent summary of the potential, current progress, and problems of deep learning (DL) approaches to ToM. We highlight that many current findings can be explained through shortcuts. These shortcuts arise because the tasks used to investigate ToM in deep learning systems have been too narrow. Thus, we encourage researchers to investigate ToM in complex open-ended environments. Furthermore, to inspire future DL systems we provide a concise overview of prior work done in humans. We further argue that when studying ToM with DL, the research’s main focus and contribution ought to be opening up the network’s representations. We recommend researchers to use tools from the field of interpretability of AI to study the relationship between different network components and aspects of ToM.},
  AUTHOR = {Aru, Jaan and Labash, Aqeel and Corcoll, Oriol and Vicente, Raul},
  LANGUAGE = {en},
  URL = {https://doi.org/10.1007/s10462-023-10401-x},
  DATE = {2023-09},
  DOI = {10.1007/s10462-023-10401-x},
  FILE = {Version soumise:/Users/bonard/switchdrive2/Zotero/storage/RE976DXH/Aru et al. - 2023 - Mind the gap challenges of deep learning approach.pdf:application/pdf},
  ISSN = {1573-7462},
  JOURNALTITLE = {Artificial Intelligence Review},
  KEYWORDS = {Artificial intelligence,Deep learning,Reinforcement learning,Theory of Mind},
  NUMBER = {9},
  PAGES = {9141--9156},
  SHORTTITLE = {Mind the gap},
  TITLE = {Mind the gap: challenges of deep learning approaches to {Theory} of {Mind}},
  URLDATE = {2023-12-08},
  VOLUME = {56},
}

@ARTICLE{aviezer_angry_2008,
  AUTHOR = {Aviezer, Hillel and Hassin, Ran R. and Ryan, Jennifer and Grady, Cheryl and Susskind, Josh and Anderson, Adam and Moscovitch, Morris and Bentin, Shlomo},
  DATE = {2008},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/VGGJHA39/Aviezer et al. - 2008 - Angry, disgusted, or afraid Studies on the mallea.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/YVRF99XI/j.1467-9280.2008.02148.html:text/html},
  JOURNALTITLE = {Psychological science},
  NOTE = {Publisher: SAGE Publications Sage CA: Los Angeles, CA},
  NUMBER = {7},
  PAGES = {724--732},
  SHORTTITLE = {Angry, disgusted, or afraid?},
  TITLE = {Angry, disgusted, or afraid? {Studies} on the malleability of emotion perception},
  VOLUME = {19},
}

@ARTICLE{banksSystemicFunctionalLinguistics2002a,
  ABSTRACT = {Although different theoretical models are complementary, functional models in general, and Systemic Functional Linguistics in particular lend themselves to the analysis of text. This is so at the level of register, in terms of Field, Tenor and Mode, as well as at the level of semantics in terms of ideational, interpersonal and textual metafunction. This paper will consider two abstracts for research articles, one from the area of particle physics, the other, ESP. These abstracts are very similar in terms of Field, Tenor and Mode, since the semiotics of research publishing functions in virtually the same way in these two disciplines. In terms of the semantic metafunctions, it is in the interpersonal metafunction that they differ least, as they function in the same way. Differences in the textual and ideational metafunctions seem to be more significant. My object here is not to claim that these two abstracts provide results which are typical for their sub-genres, but to demonstrate that Systemic Functional Linguistics provides a useful theoretical framework for the purposes of text analysis.},
  AUTHOR = {Banks, David},
  DATE = {2002},
  FILE = {C:\Users\Gustave\Zotero\storage\VKPK2UXH\Banks - 2002 - Systemic Functional Linguistics as a model for tex.pdf},
  ISSN = {1246-8185},
  JOURNALTITLE = {ASp. la revue du GERAS},
  KEYWORDS = {abstract,register,semantic metafunction,SFL,systemic functional linguistics,text analysis},
  LANGID = {english},
  NUMBER = {35},
  PAGES = {23--34},
  TITLE = {Systemic {{Functional Linguistics}} as a Model for Text Analysis},
}

@BOOK{banksSystemicFunctionalGrammar2019b,
  AUTHOR = {Banks, David},
  PUBLISHER = {Routledge},
  DATE = {2019},
  ISBN = {978-0-429-46784-4},
  LANGID = {english},
  SHORTTITLE = {A Systemic Functional Grammar of English},
  TITLE = {A Systemic Functional Grammar of English: A Simple Introduction},
}

@ARTICLE{barrett_context_2011,
  AUTHOR = {Barrett, Lisa Feldman and Mesquita, Batja and Gendron, Maria},
  DATE = {2011},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/56I538CI/Barrett et al. - 2011 - Context in emotion perception.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/6SVWXBWX/0963721411422522.html:text/html},
  JOURNALTITLE = {Current Directions in Psychological Science},
  NOTE = {Publisher: Sage Publications Sage CA: Los Angeles, CA},
  NUMBER = {5},
  PAGES = {286--290},
  TITLE = {Context in emotion perception},
  VOLUME = {20},
}

@ARTICLE{barrett_emotional_2019,
  ABSTRACT = {It is commonly assumed that a person’s emotional state can be readily inferred from his or her facial movements, typically called emotional expressions or facia...},
  AUTHOR = {Barrett, Lisa Feldman and Adolphs, Ralph and Marsella, Stacy and Martinez, Aleix M. and Pollak, Seth D.},
  LANGUAGE = {en},
  URL = {https://journals.sagepub.com/doi/10.1177/1529100619832930},
  DATE = {2019-07},
  DOI = {10.1177/1529100619832930},
  FILE = {Barrett et al 2019 emotional expressions reconsidered.pdf:/Users/bonard/switchdrive2/projets académiques recherches/my articles/_projects/why AI can't recognize emotional expressions/Barrett et al 2019 emotional expressions reconsidered.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/WMPZJF7C/1529100619832930.html:text/html;Version acceptée:/Users/bonard/switchdrive2/Zotero/storage/TUUKDXPM/Barrett et al. - 2019 - Emotional Expressions Reconsidered Challenges to .pdf:application/pdf},
  JOURNALTITLE = {Psychological Science in the Public Interest},
  NOTE = {Publisher: SAGE PublicationsSage CA: Los Angeles, CA},
  SHORTTITLE = {Emotional {Expressions} {Reconsidered}},
  TITLE = {Emotional expressions reconsidered: {Challenges} to inferring emotion from human facial movements},
  URLDATE = {2020-05-17},
}

@BOOK{beckCognitiveTherapyEmotional1976,
  ABSTRACT = {Traces the development of the cognitive approach to psychopathology and psy hotherapy from common-sense observations and folk wisdom, to a more sophisticated understanding of the emotional disorders, and finally to the application of rational techniques to correct the misconceptions and conceptual distortions that form the matrix of the neuroses. The importance of engaging the patient in exploration of his inner world and of obtaining a sharp delineation of specific thoughts and underlying assumptions is emphasized. (91/4 p ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  AUTHOR = {Beck, Aaron T.},
  LOCATION = {Oxford, England},
  PUBLISHER = {International Universities Press},
  DATE = {1976},
  KEYWORDS = {Cognitive Processes,Emotional Disturbances,Psychotherapy},
  PAGES = {356},
  SERIES = {Cognitive Therapy and the Emotional Disorders},
  TITLE = {Cognitive Therapy and the Emotional Disorders},
}

@ARTICLE{berstelOriginsCombinatoricsWords2007,
  ABSTRACT = {We investigate the historical roots of the field of combinatorics on words. They comprise applications and interpretations in algebra, geometry and combinatorial enumeration. These considerations gave rise to early results such as those of Axel Thue at the beginning of the 20th century. Other early results were obtained as a by-product of investigations on various combinatorial objects. For example, paths in graphs are encoded by words in a natural way, and conversely, the Cayley graph of a group or a semigroup encodes words by paths. We give in this text an account of this two-sided interaction.},
  AUTHOR = {Berstel, Jean and Perrin, Dominique},
  URL = {https://www.sciencedirect.com/science/article/pii/S0195669805001629},
  DATE = {2007-04},
  DOI = {10.1016/j.ejc.2005.07.019},
  FILE = {C\:\\Users\\Gustave\\Zotero\\storage\\29HLDMI8\\Berstel et Perrin - 2007 - The origins of combinatorics on words.pdf;C\:\\Users\\Gustave\\Zotero\\storage\\KZTICHEF\\S0195669805001629.html},
  ISSN = {0195-6698},
  JOURNALTITLE = {European Journal of Combinatorics},
  NUMBER = {3},
  PAGES = {996--1022},
  TITLE = {The Origins of Combinatorics on Words},
  URLDATE = {2025-08-01},
  VOLUME = {28},
}

@MISC{bertolini2023automatic,
  AUTHOR = {Bertolini, Lorenzo and Elce, Valentina and Michalak, Adriana and Bernardi, Giulio and Weeds, Julie},
  DATE = {2023},
  EPRINT = {2302.14828 [cs.CL]},
  EPRINTTYPE = {arxiv},
  TITLE = {Automatic scoring of dream reports' emotional content with large language models},
}

@ARTICLE{blancComprehensionContesEntre2010,
  ABSTRACT = {Orally presented tales comprehension was studied in 5-to-7-year-old children using a sentence verification task. The sentences to be judged by children concerned the emotional dimension of the described situations, but also their spatial, temporal, and causal dimensions, as well as the characters involved. In addition to observing a better tales comprehension with increasing age, we found that the emotional dimension was represented with less precision than the causal or spatial dimensions. A deeper examination of the emotional dimension representation revealed that the precision of the representation depends on the type of emotional information that is presented, (i.e., the emotion is designed, the behavioral expression of an emotion is described, an event that generates the emotion is mentioned), but it also depends on the gender and age. The diverse implications of this study are discussed in the light of data coming from the literature on comprehension skills in children. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  AUTHOR = {Blanc, Nathalie},
  LOCATION = {US},
  PUBLISHER = {Educational Publishing Foundation},
  DATE = {2010},
  DOI = {10.1037/a0021283},
  ISSN = {1878-7290},
  JOURNALTITLE = {Canadian Journal of Experimental Psychology / Revue canadienne de psychologie expérimentale},
  KEYWORDS = {Comprehension,Emotions,Inference,Storytelling},
  NUMBER = {4},
  PAGES = {256--265},
  SHORTTITLE = {La Compréhension Des Contes Entre 5 et 7 Ans},
  TITLE = {La Compréhension Des Contes Entre 5 et 7 Ans: {{Quelle}} Représentation Des Informations Émotionnelles? [{{The}} Comprehension of the Tales between 5 and 7 Year-Olds: {{Which}} Representation of Emotional Information?]},
  VOLUME = {64},
}

@ARTICLE{blanc_production_2017,
  AUTHOR = {Blanc, Nathalie and Quenette, Guy},
  URL = {https://www.cairn.info/revue-enfance2-2017-4-page-503.htm},
  DATE = {2017},
  JOURNALTITLE = {Enfance},
  NOTE = {Publisher: NecPlus},
  NUMBER = {4},
  PAGES = {503--511},
  SHORTTITLE = {La production d’inférences émotionnelles entre 8 et 10 ans},
  TITLE = {La production d’inférences émotionnelles entre 8 et 10 ans: quelle méthodologie pour quels résultats?},
  URLDATE = {2024-02-13},
  VOLUME = {4},
}

@ARTICLE{bonard_emotions_2021,
  ABSTRACT = {Cette discussion examine plusieurs façons de comprendre les émotions comme des réactions évaluatives. Il existe un consensus dans les sciences affectives qui veut que les émotions paradigmatiques soient faites de quatre composants : catégorisation du stimulus, tendances à l’action, changements corporels et aspect phénoménal. L’article expose les quatre principales théories dans la philosophie contemporaine des émotions et montre qu’elles ont tendance à se focaliser sur l’un ou l’autre des quatre composants des émotions pour expliquer leur nature évaluative. La conclusion est qu’il est possible de rendre compte des émotions comme réactions évaluatives à ces quatre niveaux et que, pour cette raison, les conceptions présentées sont plus complémentaires qu’on ne le suppose généralement.},
  AUTHOR = {Bonard, Constant},
  LANGUAGE = {FR},
  URL = {https://www.cairn.info/revue-de-metaphysique-et-de-morale-2021-2-page-209.htm},
  DATE = {2021},
  DOI = {10.3917/rmm.212.0209},
  ISSN = {9782130828389},
  JOURNALTITLE = {Revue de métaphysique et de morale},
  NOTE = {Place: Paris cedex 14 Publisher: Presses Universitaires de France},
  NUMBER = {2},
  PAGES = {209--229},
  TITLE = {Émotions et sensibilité aux valeurs : quatre conceptions philosophiques contemporaines},
  VOLUME = {110},
}

@THESIS{bonard_meaning_2021,
  AUTHOR = {Bonard, Constant},
  INSTITUTION = {University of Geneva and University of Antwerp},
  URL = {https://doi.org/10.13097/archive-ouverte/unige:150524},
  DATE = {2021},
  TITLE = {Meaning and emotion: {The} extended {Gricean} model and what emotional signs mean},
  TYPE = {Doctoral dissertation},
}

@ARTICLE{bonard_beyond_2022,
  ABSTRACT = {In this paper, I am going to cast doubt on an idea that is shared, explicitly or implicitly, by most contemporary pragmatic theories: that the inferential interpretation procedure described by Grice, neo-Griceans, or post-Griceans applies only to the interpretation of ostensive stimuli. For this special issue, I will concentrate on the relevance theory (RT) version of this idea. I will proceed by putting forward a dilemma for RT and argue that the best way out of it is to accept that the relevance-theoretic comprehension procedure applies to certain non-ostensive stimuli, contrary to what is generally claimed within RT. In particular, I will argue that relevance theorists should accept that (ceteris paribus) non-ostensive emotional expressions in interactions guarantee a presumption of relevance such that they are interpreted through the relevance-theoretic comprehension procedure. This leads me to propose what I call 'the expressive principle of relevance'.},
  AUTHOR = {Bonard, Constant},
  LANGUAGE = {en},
  URL = {https://www.sciencedirect.com/science/article/pii/S0378216621003647},
  DATE = {2022},
  DOI = {10.1016/j.pragma.2021.10.024},
  FILE = {ScienceDirect Full Text PDF:/Users/bonard/switchdrive2/Zotero/storage/SV4VEY3W/Bonard - 2022 - Beyond ostension Introducing the expressive princ.pdf:application/pdf},
  ISSN = {0378-2166},
  JOURNALTITLE = {Journal of Pragmatics},
  KEYWORDS = {Emotional expression,Non-ostensive,Ostension,Relevance theory,Relevance-theoretic comprehension procedure,Unintentional communication},
  PAGES = {13--23},
  SHORTTITLE = {Beyond ostension},
  TITLE = {Beyond ostension: {Introducing} the expressive principle of relevance},
  URLDATE = {2021-11-19},
  VOLUME = {187},
}

@ARTICLE{bonard_natural_2023,
  AUTHOR = {Bonard, Constant},
  DATE = {2023},
  DOI = {https://doi.org/10.1007/s11229-023-04144-z},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/C38A9CDQ/Bonard - 2023 - Natural meaning, probabilistic meaning, and the in.doc:application/msword},
  JOURNALTITLE = {Synthese},
  NOTE = {Publisher: Springer},
  NUMBER = {5},
  PAGES = {167},
  TITLE = {Natural meaning, probabilistic meaning, and the interpretation of emotional signs},
  VOLUME = {201},
}

@ARTICLE{bonard_underdeterminacy_2023,
  AUTHOR = {Bonard, Constant},
  DATE = {2023},
  DOI = {https://doi.org/10.1111/mila.12481},
  JOURNALTITLE = {Mind \& Language},
  TITLE = {Underdeterminacy without ostension: {A} blind spot in the prevailing models of communication},
}

@INCOLLECTION{bonard_can_2024,
  AUTHOR = {Bonard, Constant},
  EDITOR = {Strasser, Anna},
  LOCATION = {Berlin},
  PUBLISHER = {Xenemoi},
  BOOKTITLE = {Anna's {AI} {Anthology}. {How} to live with smart machines?},
  DATE = {2024},
  TITLE = {Can {AI} and humans genuinely communicate?},
}

@INPROCEEDINGS{bonard-cortal-2024-improving,
  ABSTRACT = {We propose leveraging cognitive science research on emotions and communication to improve language models for emotion analysis. First, we present the main emotion theories in psychology and cognitive science. Then, we introduce the main methods of emotion annotation in natural language processing and their connections to psychological theories. We also present the two main types of analyses of emotional communication in cognitive pragmatics. Finally, based on the cognitive science research presented, we propose directions for improving language models for emotion analysis. We suggest that these research efforts pave the way for constructing new annotation schemes, methods, and a possible benchmark for emotional understanding, considering different facets of human emotion and communication.},
  AUTHOR = {Bonard, Constant and Cortal, Gustave},
  EDITOR = {Kuribayashi, Tatsuki and Rambelli, Giulia and Takmaz, Ece and Wicke, Philipp and Oseki, Yohei},
  LOCATION = {Bangkok, Thailand},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2024.cmcl-1.23/},
  BOOKTITLE = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  DATE = {2024-08},
  DOI = {10.18653/v1/2024.cmcl-1.23},
  PAGES = {264--277},
  TITLE = {Improving Language Models for Emotion Analysis: {{Insights}} from Cognitive Science},
}

@INCOLLECTION{bonard_emotion_2023,
  AUTHOR = {Bonard, Constant and Deonna, Julien},
  EDITOR = {Schiewer, Gesine Lenore and Altarriba, Jeanette and Ng, Bee Chin},
  LOCATION = {Berlin},
  PUBLISHER = {de Gruyter},
  URL = {https://doi.org/10.1515/9783110347524-003},
  BOOKTITLE = {Language and emotion: {An} international handbook},
  DATE = {2023},
  PAGES = {54--72},
  TITLE = {Emotion and language in philosophy},
  VOLUME = {1},
}

@INPROCEEDINGS{bostan-etal-2020-goodnewseveryone,
  AUTHOR = {Bostan, Laura Ana Maria and Kim, Evgeny and Klinger, Roman},
  LANGUAGE = {English},
  LOCATION = {Marseille, France},
  PUBLISHER = {European Language Resources Association},
  URL = {https://aclanthology.org/2020.lrec-1.194},
  BOOKTITLE = {Proceedings of the 12th Language Resources and Evaluation Conference},
  DATE = {2020-05},
  ISBN = {979-10-95546-34-4},
  PAGES = {1554--1566},
  TITLE = {{G}ood{N}ews{E}veryone: A Corpus of News Headlines Annotated with Emotions, Semantic Roles, and Reader Perception},
}

@INPROCEEDINGS{bostan-klinger-2018-analysis,
  AUTHOR = {Bostan, Laura-Ana-Maria and Klinger, Roman},
  LOCATION = {Santa Fe, New Mexico, USA},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/C18-1179},
  BOOKTITLE = COLING:2018:1,
  DATE = {2018-08},
  PAGES = {2104--2119},
  TITLE = {An Analysis of Annotated Corpora for Emotion Classification in Text},
}

@INPROCEEDINGS{brownLanguageModelsAre2020a,
  ABSTRACT = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  AUTHOR = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in {{Neural Information Processing Systems}}},
  DATE = {2020},
  PAGES = {1877--1901},
  TITLE = {Language {{Models}} Are {{Few-Shot Learners}}},
  VOLUME = {33},
}

@BOOK{brunerActsMeaning1990,
  ABSTRACT = {Bruner begins his investigations with an incisive assessment of the cognitive revolution from its historical origins to its present-day preoccupation with computers and computer-ability. . . . Bruner argues that the current fixation on mind as "information processor" has led psychology away from the deeper objective of understanding mind as a creator of meanings. Bruner develops his thesis by elaborating upon the powerful role of culturally shaped narrative thinking in our conception of ourselves and of the social world in which we live. He explores the central significance of "folk psychology" not only in determining human interaction but also in providing a basis for the social institutions that constrain it. And then he demonstrates how narrative thinking, beginning with the acquisition of language, brings the growing human being into the arena of human culture. . . . Finally, through an examination of spontaneous autobiography, he shows how the very conception of Self comes to reflect both individual idiosyncrasy and the requirements of social living. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  AUTHOR = {Bruner, Jerome},
  LOCATION = {Cambridge, MA, US},
  PUBLISHER = {Harvard University Press},
  DATE = {1990},
  FILE = {C:\Users\Gustave\Zotero\storage\Q7UBX39J\1990-98641-000.html},
  ISBN = {978-0-674-00360-6},
  KEYWORDS = {Culture (Anthropological),Meaning,Psychology,Self-Perception,Social Interaction,Social Perception},
  SERIES = {Acts of Meaning},
  TITLE = {Acts of Meaning},
}

@INPROCEEDINGS{buechel-hahn-2017-emobank,
  AUTHOR = {Buechel, Sven and Hahn, Udo},
  LOCATION = {Valencia, Spain},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/E17-2092},
  BOOKTITLE = EACL:2017:2,
  DATE = {2017-04},
  PAGES = {578--585},
  TITLE = {{E}mo{B}ank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis},
}

@INPROCEEDINGS{buechel-etal-2021-towards,
  AUTHOR = {Buechel, Sven and Modersohn, Luise and Hahn, Udo},
  LOCATION = {Online and Punta Cana, Dominican Republic},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2021.emnlp-main.728},
  BOOKTITLE = EMNLP:2021:MAIN,
  DATE = {2021-11},
  DOI = {10.18653/v1/2021.emnlp-main.728},
  PAGES = {9231--9249},
  TITLE = {Towards Label-Agnostic Emotion Embeddings},
}

@BOOK{buhlerTheoryLanguageRepresentational1990,
  AUTHOR = {Bühler, Karl},
  PUBLISHER = {John Benjamins},
  DATE = {1990},
  FILE = {C:\Users\Gustave\Zotero\storage\VX7MEJRI\BHLTOL.html},
  SHORTTITLE = {Theory of Language},
  TITLE = {Theory of Language: {{The}} Representational Function of Language},
}

@ARTICLE{Bulkeley2018,
  AUTHOR = {Bulkeley, Kelly and Graves, Mark},
  URL = {https://doi.org/10.1037/drm0000071},
  DATE = {2018-03},
  DOI = {10.1037/drm0000071},
  JOURNALTITLE = {Dreaming : journal of the Association for the Study of Dreams},
  NOTE = {Publisher: American Psychological Association ({APA})},
  NUMBER = {1},
  PAGES = {43--58},
  SHORTJOURNAL = {Dreaming},
  TITLE = {Using the {LIWC} program to study dreams.},
  VOLUME = {28},
}

@INPROCEEDINGS{cambria,
  AUTHOR = {Cambria, Erik and Li, Yang and Xing, Frank Z. and Poria, Soujanya and Kwok, Kenneth},
  LOCATION = {Virtual Event, Ireland},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://doi.org/10.1145/3340531.3412003},
  BOOKTITLE = {Proceedings of the 29th {ACM} international conference on information and knowledge management},
  DATE = {2020},
  DOI = {10.1145/3340531.3412003},
  ISBN = {978-1-4503-6859-9},
  KEYWORDS = {natural language processing,sentiment analysis,knowledge representation and reasoning},
  NOTE = {Number of pages: 10 tex.address: New York, {NY}, {USA}},
  PAGES = {105--114},
  SERIES = {Cikm '20},
  TITLE = {{SenticNet} 6: Ensemble application of symbolic and subsymbolic {AI} for sentiment analysis},
}

@INPROCEEDINGS{campagnano-etal-2022-srl4e,
  AUTHOR = {Campagnano, Cesare and Conia, Simone and Navigli, Roberto},
  LOCATION = {Dublin, Ireland},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2022.acl-long.314},
  BOOKTITLE = ACL:2022:LONG,
  DATE = {2022-05},
  DOI = {10.18653/v1/2022.acl-long.314},
  PAGES = {4586--4601},
  TITLE = {{SRL4E} {--} {S}emantic {R}ole {L}abeling for {E}motions: {A} Unified Evaluation Framework},
}

@ARTICLE{carhart-harrisEntropicBrainRevisited2018,
  ABSTRACT = {The entropic brain hypothesis proposes that within upper and lower limits, after which consciousness may be lost, the entropy of spontaneous brain activity indexes the informational richness of conscious states. Here the hypothesis is revisited four years on from its original publication. It is shown that the principle that the entropy of brain activity is elevated in the psychedelic state is increasingly well supported by separate and independent studies and analyses, and evidence for greater brain criticality under psychedelics is also highlighted. It is argued that heightened brain criticality enables the brain to be more sensitive to intrinsic and extrinsic perturbations which may translate as a heightened susceptibility to “set” and “setting”. This updated version of the original entropic brain hypothesis now offers more concrete information on specific measures of brain entropy and suggests new studies to scrutinise it further, as well as examine its utility for describing and informing the treatment of psychiatric and neurological conditions such as depression and disorders of consciousness. This article is part of the Special Issue entitled ‘Psychedelics: New Doors, Altered Perceptions’.},
  AUTHOR = {Carhart-Harris, Robin L.},
  DATE = {2018},
  FILE = {C:\Users\Gustave\Zotero\storage\JPRC2ZBH\S0028390818301175.html},
  ISSN = {0028-3908},
  JOURNALTITLE = {Neuropharmacology},
  KEYWORDS = {5-HT2A,Criticality,Depression,Entropy,Psychedelics,Serotonin},
  PAGES = {167--178},
  SERIES = {Psychedelics: {{New}} Doors, Altered Perceptions},
  TITLE = {The Entropic Brain - Revisited},
  VOLUME = {142},
}

@INCOLLECTION{emotionregulation1,
  AUTHOR = {Cartwright, Rosalind},
  EDITOR = {Kryger, Meir H. and Roth, Thomas and Dement, William C.},
  LOCATION = {Philadelphia},
  PUBLISHER = {W.B. Saunders},
  URL = {https://www.sciencedirect.com/science/article/pii/B0721607977500525},
  BOOKTITLE = {Principles and practice of sleep medicine (fourth edition)},
  DATE = {2005},
  DOI = {https://doi.org/10.1016/B0-72-160797-7/50052-5},
  EDITION = {Fourth Edition},
  ISBN = {978-0-7216-0797-9},
  PAGES = {565--572},
  TITLE = {Chapter 45 - dreaming as a mood regulation system},
}

@INPROCEEDINGS{casel-etal-2021-emotion,
  AUTHOR = {Casel, Felix and Heindl, Amelie and Klinger, Roman},
  LOCATION = {Düsseldorf, Germany},
  PUBLISHER = {KONVENS 2021 Organizers},
  URL = {https://aclanthology.org/2021.konvens-1.5},
  BOOKTITLE = KONVENS:2021:1,
  DATE = {2021-06},
  PAGES = {49--61},
  TITLE = {Emotion Recognition under Consideration of the Emotion Component Process Model},
}

@INPROCEEDINGS{chen-etal-2020-unified,
  AUTHOR = {Chen, Xinhong and Li, Qing and Wang, Jianping},
  LOCATION = {Barcelona, Spain (Online)},
  PUBLISHER = {International Committee on Computational Linguistics},
  URL = {https://aclanthology.org/2020.coling-main.18},
  BOOKTITLE = COLING:2020:MAIN,
  DATE = {2020-12},
  DOI = {10.18653/v1/2020.coling-main.18},
  PAGES = {208--218},
  TITLE = {A Unified Sequence Labeling Model for Emotion Cause Pair Extraction},
}

@INPROCEEDINGS{chen-etal-2010-emotion,
  AUTHOR = {Chen, Ying and Lee, Sophia Yat Mei and Li, Shoushan and Huang, Chu-Ren},
  LOCATION = {Beijing, China},
  PUBLISHER = {Coling 2010 Organizing Committee},
  URL = {https://aclanthology.org/C10-1021},
  BOOKTITLE = {Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)},
  DATE = {2010-08},
  PAGES = {179--187},
  TITLE = {Emotion Cause Detection with Linguistic Constructions},
}

@INPROCEEDINGS{chenEmpoweringPsychotherapyLarge2023a,
  ABSTRACT = {Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy. We study the task of cognitive distortion detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs diagnosis on the patient's speech via three stages: subjectivity assessment to separate the facts and the thoughts; contrastive reasoning to elicit the reasoning processes supporting and contradicting the thoughts; and schema analysis to summarize the cognition schemas. The generated diagnosis rationales through the three stages are essential for assisting the professionals. Experiments demonstrate that DoT obtains significant improvements over ChatGPT for cognitive distortion detection, while generating high-quality rationales approved by human experts.},
  AUTHOR = {Chen, Zhiyu and Lu, Yujie and Wang, William},
  EDITOR = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  LOCATION = {Singapore},
  PUBLISHER = ACL,
  BOOKTITLE = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  DATE = {2023-12},
  DOI = {10.18653/v1/2023.findings-emnlp.284},
  PAGES = {4295--4304},
  SHORTTITLE = {Empowering {{Psychotherapy}} with {{Large Language Models}}},
  TITLE = {Empowering {{Psychotherapy}} with {{Large Language Models}}: {{Cognitive Distortion Detection}} through {{Diagnosis}} of {{Thought Prompting}}},
}

@INPROCEEDINGS{cortal-2024-sequence-sequence,
  AUTHOR = {Cortal, Gustave},
  EDITOR = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
  LOCATION = {Torino, Italia},
  PUBLISHER = {ELRA and ICCL},
  URL = {https://aclanthology.org/2024.lrec-main.1282},
  BOOKTITLE = LREC:2024:MAIN,
  DATE = {2024-05},
  PAGES = {14717--14728},
  TITLE = {Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives},
}

@INPROCEEDINGS{cortal-2024-sequence,
  ABSTRACT = {The study of dreams has been central to understanding human (un)consciousness, cognition, and culture for centuries. Analyzing dreams quantitatively depends on labor-intensive, manual annotation of dream narratives. We automate this process through a natural language sequence-to-sequence generation framework. This paper presents the first study on character and emotion detection in the English portion of the open DreamBank corpus of dream narratives. Our results show that language models can effectively address this complex task. To get insight into prediction performance, we evaluate the impact of model size, prediction order of characters, and the consideration of proper names and character traits. We compare our approach with a large language model using in-context learning. Our supervised models perform better while having 28 times fewer parameters. Our model and its generated annotations are made publicly available.},
  AUTHOR = {Cortal, Gustave},
  EDITOR = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
  LOCATION = {Torino, Italia},
  PUBLISHER = {ELRA and ICCL},
  URL = {https://aclanthology.org/2024.lrec-main.1282/},
  BOOKTITLE = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation ({{LREC-COLING}} 2024)},
  DATE = {2024-05},
  PAGES = {14717--14728},
  TITLE = {Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives},
}

@MISC{cortalNaturalLanguageProcessing2022,
  ABSTRACT = {Emotion analysis in texts suffers from two major limitations: annotated gold-standard corpora are mostly small and homogeneous, and emotion identification is often simplified as a sentence-level classification problem. To address these issues, we introduce a new annotation scheme for exploring emotions and their causes, along with a new French dataset composed of autobiographical accounts of an emotional scene. The texts were collected by applying the Cognitive Analysis of Emotions developed by A. Finkel to help people improve on their emotion management. The method requires the manual analysis of an emotional event by a coach trained in Cognitive Analysis. We present a rule-based approach to automatically annotate emotions and their semantic roles (e.g. emotion causes) to facilitate the identification of relevant aspects by the coach. We investigate future directions for emotion analysis using graph structures.},
  AUTHOR = {Cortal, Gustave and Finkel, Alain and Paroubek, Patrick and Ye, Lina},
  ORGANIZATION = {arXiv},
  DATE = {2022-10},
  DOI = {10.48550/arXiv.2210.05296},
  EPRINT = {2210.05296},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Computation and Language},
  TITLE = {Natural {{Language Processing}} for {{Cognitive Analysis}} of {{Emotions}}},
}

@INPROCEEDINGS{cortal:hal-03805702,
  AUTHOR = {Cortal, Gustave and Finkel, Alain and Paroubek, Patrick and Ye, Lina},
  LOCATION = {Paris, France},
  URL = {https://hal.inria.fr/hal-03805702},
  BOOKTITLE = {Semantics, memory, and emotion 2022},
  DATE = {2022-09},
  KEYWORDS = {Sentiment Analysis,Natural Language Processing,Cognitive Analysis of Emotions,Emotion Analysis},
  NOTE = {tex.hal\_id: hal-03805702 tex.hal\_version: v1},
  TITLE = {Natural language processing for cognitive analysis of emotions},
}

@INPROCEEDINGS{cortal2022natural,
  AUTHOR = {Cortal, Gustave and Finkel, Alain and Paroubek, Patrick and Ye, Lina},
  LOCATION = {Paris, France},
  URL = {https://inria.hal.science/hal-03805702},
  BOOKTITLE = {Semantics, memory, and emotion 2022},
  DATE = {2022-09},
  KEYWORDS = {Sentiment Analysis,Natural Language Processing,Cognitive Analysis of Emotions,Emotion Analysis},
  NOTE = {tex.hal\_id: hal-03805702 tex.hal\_version: v1},
  TITLE = {Natural language processing for cognitive analysis of emotions},
}

@INPROCEEDINGS{cortal-etal-2023-emotion,
  AUTHOR = {Cortal, Gustave and Finkel, Alain and Paroubek, Patrick and Ye, Lina},
  EDITOR = {Degaetano-Ortlieb, Stefania and Kazantseva, Anna and Reiter, Nils and Szpakowicz, Stan},
  LOCATION = {Dubrovnik, Croatia},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2023.latechclfl-1.8},
  BOOKTITLE = LATECHCLFL:2023:1,
  DATE = {2023-05},
  DOI = {10.18653/v1/2023.latechclfl-1.8},
  PAGES = {72--81},
  TITLE = {Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation},
}

@ARTICLE{cortes1995support,
  AUTHOR = {Cortes, Corinna and Vapnik, Vladimir},
  DATE = {1995},
  JOURNALTITLE = {Machine learning},
  NOTE = {Publisher: Springer},
  NUMBER = {3},
  PAGES = {273--297},
  TITLE = {Support-vector networks},
  VOLUME = {20},
}

@INCOLLECTION{cosmides_evolutionary_2000,
  AUTHOR = {Cosmides, Leda and Tooby, John},
  EDITOR = {Lewis, Michael and Haviland-Jones, Jeannette M.},
  LOCATION = {New York},
  PUBLISHER = {Guilford Press},
  BOOKTITLE = {Handbook of emotions},
  DATE = {2000},
  EDITION = {2nd},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/LF697XF2/Cosmides et Tooby - 2000 - Evolutionary psychology and the emotions.pdf:application/pdf},
  NOTE = {Publisher: Citeseer},
  PAGES = {91--115},
  TITLE = {Evolutionary psychology and the emotions},
}

@INPROCEEDINGS{costetchi2022graph,
  AUTHOR = {Costetchi, Eugeniu},
  LOCATION = {Prague, Czech Republic},
  PUBLISHER = {Charles University in Prague, Matfyzpress},
  URL = {https://aclanthology.org/W13-3709},
  BOOKTITLE = {Proceedings of the 2nd International Conference on Dependency Linguistics ({{DepLing}}\,2013)},
  DATE = {2013},
  PAGES = {68--77},
  TITLE = {A Method to Generate Simplified Systemic Functional Parses from Dependency Parses},
}

@ARTICLE{creissenQuelleRepresentationDifferentes2017a,
  ABSTRACT = {Résumé La présente étude examine comment les enfants âgés de 6 à 10ans comprennent et se représentent les différentes facettes de la dimension émotionnelle d'une histoire. Le caractère généralisable des capacités de compréhension des enfants est également mis à l'épreuve en comparant leur compréhension en situation auditive (i.e., histoire à écouter) et en situation audiovisuelle (i.e., histoire télévisée). À travers l'exploration de leurs habiletés à se représenter les passages de l'histoire où l'émotion du personnage est soit explicitement désignée, soit exprimée à travers son comportement, soit suggérée par l'événement décrit, cette étude permet de dégager trois principaux apports : (1) les performances des enfants en situations auditive et audiovisuelle étant corrélées, le caractère généralisable des capacités de compréhension est confirmé ; (2) la représentation de certaines facettes de la dimension émotionnelle gagne en précision lorsqu'elle est appréhendée à partir du support audiovisuel ; (3) entre 6 et 10ans, les enfants exploitent davantage les informations émotionnelles explicites que les informations émotionnelles implicites. Ces résultats sont discutés au regard des études antérieures menées sur la compréhension de récits chez l'enfant. This study examined how children between ages 6 and 10 understand and represent the different facets of the emotional dimension of a story. The idea that children's comprehension skills generalize across different media was also tested by comparing their ability to understand auditory and televised stories. Based on the exploration of their ability to represent the story passages where the emotion of the character was either explicitly mentioned, or expressed through his/her behavior, or suggested by the event described, this study highlighted three main contributions: (1) overall, children's comprehension skills generalize across different media; (2) some facets of the emotional dimension of a story were better represented from the audiovisual presentation compared to the auditory one (3) between the ages of 6 and 10 years, explicit emotional information was easier to use than implicit emotional information. These results are discussed in relation to the findings of previous studies conducted on children's understanding of story.},
  AUTHOR = {Creissen, S. and Blanc, N.},
  DATE = {2017-09},
  DOI = {10.1016/j.psfr.2015.07.006},
  ISSN = {0033-2984},
  JOURNALTITLE = {Psychologie Française},
  KEYWORDS = {Développement de la compréhension de récits,Development of story understanding,Dimension émotionnelle,Emotional dimension,Génération d'inférences,Inference generation,Modalité de présentation,Modality of presentation},
  NUMBER = {3},
  PAGES = {263--277},
  SERIES = {Cognition et Multimédia : {{Les}} Atouts Du Numérique En Situation d'apprentissage},
  SHORTTITLE = {Quelle Représentation Des Différentes Facettes de La Dimension Émotionnelle d'une Histoire Entre l'âge de 6 et 10 Ans ?},
  TITLE = {Quelle Représentation Des Différentes Facettes de La Dimension Émotionnelle d'une Histoire Entre l'âge de 6 et 10 Ans ? {{Apports}} d'une Étude Multimédia},
  VOLUME = {62},
}

@ARTICLE{selectiveforgetting1,
  AUTHOR = {Crick, Francis and Mitchison, Graeme},
  URL = {https://doi.org/10.1038/304111a0},
  DATE = {1983-07},
  DOI = {10.1038/304111a0},
  JOURNALTITLE = {Nature},
  NOTE = {Publisher: Springer Science and Business Media {LLC}},
  NUMBER = {5922},
  PAGES = {111--114},
  TITLE = {The function of dream sleep},
  VOLUME = {304},
}

@ARTICLE{selectiveforgetting2,
  AUTHOR = {Crick, Francis and Mitchison, Graeme},
  URL = {https://doi.org/10.1016/0166-4328(95)00006-f},
  DATE = {1995-07},
  DOI = {10.1016/0166-4328(95)00006-f},
  JOURNALTITLE = {Behavioural Brain Research},
  NOTE = {Publisher: Elsevier {BV}},
  NUMBER = {1},
  PAGES = {147--155},
  TITLE = {{REM} sleep and neural nets},
  VOLUME = {69},
}

@BOOK{darwin_expression_1872,
  AUTHOR = {Darwin, Charles},
  LOCATION = {London},
  PUBLISHER = {John Murray},
  DATE = {1872},
  TITLE = {The expression of the emotions in man and animals},
}

@ARTICLE{davidsonChildrenRecallEmotional2001,
  ABSTRACT = {Determined if children were able to judge the importance of the emotional occurrences (EOs) and examined children's memory for emotional behaviours (EBs), emotional labels (ELs), and nonemotional behaviours (NBs) across 3 studies. Ss were 150 1st-, 3rd-, and 5th-graders (aged 5–11 yrs). Study 1 involved EO judgements by 10 Ss from each grade level (GL). Ss were asked 6 questions (2 reflecting happiness (HS), 2 reflecting sadness (SS), and 2 reflecting anger (AR) of which they were to choose a statement (reflecting a high emotional, HE, or low emotional, LE, situation) that would result in more HS, SS, or AR. Study 2 involved 20 Ss from each GL who responded to 2 HE and 2 LE stories constructed about everyday family life. Each story consisted of 3 emotional events that reflected HS, SS, and AR. After each story, Ss were to recall all that they could. Study 3 involved 20 Ss from each GL who responded to 2 stories in which both HEs and LEs were included in addition to the 3 types of emotions. Ss were either given or not given corresponding labels. Ss were to recall all that they could. The results show that children can readily distinguish between EBs in terms of significance and importance. Further, all Ss recalled more EBs than NBs. Also, the use of ELs appears to enhance recall. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  AUTHOR = {Davidson, Denise and Luo, Zupei and Burden, Matthew J.},
  LOCATION = {United Kingdom},
  PUBLISHER = {Taylor \& Francis},
  DATE = {2001},
  DOI = {10.1080/0269993004200105},
  ISSN = {1464-0600},
  JOURNALTITLE = {Cognition and Emotion},
  KEYWORDS = {Age Differences,Behavior,Emotional Content,Judgment,Labeling,Memory,Recall (Learning)},
  NUMBER = {1},
  PAGES = {1--26},
  SHORTTITLE = {Children's Recall of Emotional Behaviours, Emotional Labels, and Nonemotional Behaviours},
  TITLE = {Children's Recall of Emotional Behaviours, Emotional Labels, and Nonemotional Behaviours: {{Does}} Emotion Enhance Memory?},
  VOLUME = {15},
}

@INPROCEEDINGS{de-bruyne-etal-2020-emotional,
  AUTHOR = {De Bruyne, Luna and De Clercq, Orphee and Hoste, Veronique},
  LANGUAGE = {English},
  LOCATION = {Marseille, France},
  PUBLISHER = {European Language Resources Association},
  URL = {https://aclanthology.org/2020.lrec-1.204},
  BOOKTITLE = {Proceedings of the 12th Language Resources and Evaluation Conference},
  DATE = {2020-05},
  ISBN = {979-10-95546-34-4},
  PAGES = {1643--1651},
  TITLE = {An Emotional Mess! Deciding on a Framework for Building a {D}utch Emotion-Annotated Corpus},
}

@INPROCEEDINGS{delestre:hal-03674695,
  AUTHOR = {Delestre, Cyrile and Amar, Abibatou},
  LOCATION = {Vannes, France},
  URL = {https://hal.archives-ouvertes.fr/hal-03674695},
  BOOKTITLE = {{CAp} (Conférence sur l'Apprentissage automatique)},
  DATE = {2022-07},
  KEYWORDS = {{CamemBERT},Distillation,{NLP},Transformers},
  NOTE = {tex.hal\_id: hal-03674695 tex.hal\_version: v1},
  TITLE = {{DistilCamemBERT} : Une distillation du modèle français {CamemBERT}},
}

@INPROCEEDINGS{demszky-etal-2020-goemotions,
  ABSTRACT = {Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce {GoEmotions}, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our {BERT}-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement.},
  AUTHOR = {Demszky, Dorottya and Movshovitz-Attias, Dana and Ko, Jeongwoo and Cowen, Alan and Nemade, Gaurav and Ravi, Sujith},
  LOCATION = {Online},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2020.acl-main.372},
  BOOKTITLE = {Proceedings of the 58th annual meeting of the association for computational linguistics},
  DATE = {2020-07},
  DOI = {10.18653/v1/2020.acl-main.372},
  PAGES = {4040--4054},
  TITLE = {{GoEmotions}: a dataset of fine-grained emotions},
}

@MISC{dengRephraseRespondLet2023c,
  ABSTRACT = {Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models (LLMs). Such discrepancies can make LLMs interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped. In this paper, we present a method named `Rephrase and Respond' (RaR), which allows LLMs to rephrase and expand questions posed by humans and provide responses in a single prompt. This approach serves as a simple yet effective prompting method for improving performance. We also introduce a two-step variant of RaR, where a rephrasing LLM first rephrases the question and then passes the original and rephrased questions together to a different responding LLM. This facilitates the effective utilization of rephrased questions generated by one LLM with another. Our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. We further provide a comprehensive comparison between RaR and the popular Chain-of-Thought (CoT) methods, both theoretically and empirically. We show that RaR is complementary to CoT and can be combined with CoT to achieve even better performance. Our work not only contributes to enhancing LLM performance efficiently and effectively but also sheds light on a fair evaluation of LLM capabilities. Data and codes are available at https://github.com/uclaml/Rephrase-and-Respond.},
  AUTHOR = {Deng, Yihe and Zhang, Weitong and Chen, Zixiang and Gu, Quanquan},
  ORGANIZATION = {arXiv},
  DATE = {2023-11},
  DOI = {10.48550/arXiv.2311.04205},
  EPRINT = {2311.04205},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  SHORTTITLE = {Rephrase and {{Respond}}},
  TITLE = {Rephrase and {{Respond}}: {{Let Large Language Models Ask Better Questions}} for {{Themselves}}},
}

@MISC{deshpandeToxicityChatGPTAnalyzing2023,
  ABSTRACT = {Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.},
  AUTHOR = {Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik},
  ORGANIZATION = {arXiv},
  DATE = {2023-04},
  DOI = {10.48550/arXiv.2304.05335},
  EPRINT = {2304.05335},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  SHORTTITLE = {Toxicity in {{ChatGPT}}},
  TITLE = {Toxicity in {{ChatGPT}}: {{Analyzing Persona-assigned Language Models}}},
}

@INPROCEEDINGS{devlin-etal-2019-bert,
  AUTHOR = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  LOCATION = {Minneapolis, Minnesota},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/N19-1423},
  BOOKTITLE = NAACL:2019:1,
  DATE = {2019-06},
  DOI = {10.18653/v1/N19-1423},
  PAGES = {4171--4186},
  TITLE = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
}

@INPROCEEDINGS{DBLP:conf/naacl/DevlinCLT19,
  AUTHOR = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  EDITOR = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  PUBLISHER = ACL,
  URL = {https://doi.org/10.18653/v1/n19-1423},
  BOOKTITLE = {Proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: Human language technologies, {NAACL}-{HLT} 2019, minneapolis, {MN}, {USA}, june 2-7, 2019, volume 1 (long and short papers)},
  DATE = {2019},
  DOI = {10.18653/V1/N19-1423},
  NOTE = {tex.bibsource: dblp computer science bibliography, https://dblp.org tex.timestamp: Mon, 26 Sep 2022 12:21:55 +0200},
  PAGES = {4171--4186},
  TITLE = {{BERT}: pre-training of deep bidirectional transformers for language understanding},
}

@ARTICLE{memoryconsolidation,
  AUTHOR = {Diekelmann, Susanne and Born, Jan},
  URL = {https://doi.org/10.1038/nrn2762},
  DATE = {2010-01},
  DOI = {10.1038/nrn2762},
  JOURNALTITLE = {Nature Reviews Neuroscience},
  NOTE = {Publisher: Springer Science and Business Media {LLC}},
  NUMBER = {2},
  PAGES = {114--126},
  TITLE = {The memory function of sleep},
  VOLUME = {11},
}

@BOOK{dilts1994strategies,
  AUTHOR = {Dilts, Robert},
  LOCATION = {Capitola, CA},
  PUBLISHER = {Meta Publications},
  URL = {https://books.google.com/books/about/Strategies_of_Genius_Aristotle_Sherlock.html?id=jAQRAQAAIAAJ},
  DATE = {1994},
  ISBN = {0-916990-32-X},
  TITLE = {Strategies of Genius, Volume 1: {{Aristotle}}, Sherlock Holmes, Walt Disney, Wolfgang Amadeus Mozart},
}

@INPROCEEDINGS{ding-etal-2020-ecpe,
  AUTHOR = {Ding, Zixiang and Xia, Rui and Yu, Jianfei},
  LOCATION = {Online},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2020.acl-main.288},
  BOOKTITLE = ACL:2020:MAIN,
  DATE = {2020-07},
  DOI = {10.18653/v1/2020.acl-main.288},
  PAGES = {3161--3170},
  TITLE = {{ECPE}-2{D}: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and Prediction},
}

@INPROCEEDINGS{ding-etal-2020-end,
  AUTHOR = {Ding, Zixiang and Xia, Rui and Yu, Jianfei},
  LOCATION = {Online},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2020.emnlp-main.290},
  BOOKTITLE = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  DATE = {2020-11},
  DOI = {10.18653/v1/2020.emnlp-main.290},
  PAGES = {3574--3583},
  TITLE = {End-to-End Emotion-Cause Pair Extraction based on Sliding Window Multi-Label Learning},
}

@BOOK{scientificstudy,
  AUTHOR = {Domhoff, G. William},
  PUBLISHER = {American Psychological Association},
  URL = {https://doi.org/10.1037/10463-000},
  DATE = {2003},
  DOI = {10.1037/10463-000},
  TITLE = {The scientific study of dreams: Neural networks, cognitive development, and content analysis.},
}

@ARTICLE{domhoffStudyingDreamContent2008a,
  AUTHOR = {Domhoff, G. William and Schneider, Adam},
  DATE = {2008},
  ISSN = {10538100},
  JOURNALTITLE = {Consciousness and Cognition},
  LANGID = {english},
  NUMBER = {4},
  PAGES = {1238--1247},
  TITLE = {Studying Dream Content Using the Archive and Search Engine on {{DreamBank}}.Net},
  VOLUME = {17},
}

@ARTICLE{dreambank,
  AUTHOR = {Domhoff, G. William and Schneider, Adam},
  URL = {https://doi.org/10.1016/j.concog.2008.06.010},
  DATE = {2008-12},
  DOI = {10.1016/j.concog.2008.06.010},
  JOURNALTITLE = {Consciousness and Cognition},
  NOTE = {Publisher: Elsevier {BV}},
  NUMBER = {4},
  PAGES = {1238--1247},
  TITLE = {Studying dream content using the archive and search engine on {DreamBank}.net},
  VOLUME = {17},
}

@ARTICLE{dong2022survey,
  AUTHOR = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Li, Lei and Sui, Zhifang},
  DATE = {2022},
  JOURNALTITLE = {{arXiv} preprint {arXiv}: 2301.00234},
  TITLE = {A survey on in-context learning},
}

@INPROCEEDINGS{dongSurveyIncontextLearning2024,
  AUTHOR = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Li, Lei and Sui, Zhifang},
  LOCATION = {Miami, Florida, USA},
  PUBLISHER = ACL,
  BOOKTITLE = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  DATE = {2024},
  LANGID = {english},
  PAGES = {1107--1128},
  TITLE = {A Survey on In-Context Learning},
}

@INPROCEEDINGS{dragos-etal-2022-angry,
  ABSTRACT = {This paper examines the role of emotion annotations to characterize extremist content released on social platforms. The analysis of extremist content is important to identify user emotions towards some extremist ideas and to highlight the root cause of where emotions and extremist attitudes merge together. To address these issues our methodology combines knowledge from sociological and linguistic annotations to explore French extremist content collected online. For emotion linguistic analysis, the solution presented in this paper relies on a complex linguistic annotation scheme. The scheme was used to annotate extremist text corpora in French. Data sets were collected online by following semi-automatic procedures for content selection and validation. The paper describes the integrated annotation scheme, the annotation protocol that was set-up for French corpora annotation and the results, e.g. agreement measures and remarks on annotation disagreements. The aim of this work is twofold: first, to provide a characterization of extremist contents; second, to validate the annotation scheme and to test its capacity to capture and describe various aspects of emotions.},
  AUTHOR = {Dragos, Valentina and Battistelli, Delphine and Etienne, Aline and Constable, Yolène},
  LOCATION = {Marseille, France},
  PUBLISHER = {European Language Resources Association},
  URL = {https://aclanthology.org/2022.lrec-1.21},
  BOOKTITLE = {Proceedings of the thirteenth language resources and evaluation conference},
  DATE = {2022-06},
  PAGES = {193--201},
  TITLE = {Angry or sad ? Emotion annotation for extremist content characterisation},
}

@ARTICLE{ekman,
  AUTHOR = {Ekman, Paul},
  URL = {https://doi.org/10.1080/02699939208411068},
  DATE = {1992},
  DOI = {10.1080/02699939208411068},
  JOURNALTITLE = {Cognition and Emotion},
  NOTE = {tex.eprint: https://doi.org/10.1080/02699939208411068},
  NUMBER = {3},
  PAGES = {169--200},
  TITLE = {An argument for basic emotions},
  VOLUME = {6},
}

@INCOLLECTION{ekman_basic_1999,
  AUTHOR = {Ekman, Paul},
  EDITOR = {Dalgleish, Tim and Power, Mike J.},
  LOCATION = {Chichester},
  PUBLISHER = {John Wiley \& Sons Ltd},
  BOOKTITLE = {Handbook of cognition and emotion},
  DATE = {1999},
  FILE = {Available Version (via Google Scholar):/Users/bonard/switchdrive2/Zotero/storage/E852PHBL/Ekman - 1999 - Basic emotions.pdf:application/pdf},
  PAGES = {45--60},
  TITLE = {Basic emotions},
  URLDATE = {2024-02-12},
}

@ARTICLE{ekman1971constantsac,
  AUTHOR = {Ekman, Paul and Friesen, W V},
  DATE = {1971},
  JOURNALTITLE = {Journal of personality and social psychology},
  PAGES = {124--9},
  TITLE = {Constants across cultures in the face and emotion.},
  VOLUME = {17 2},
}

@ARTICLE{clockssleep3030035,
  ABSTRACT = {The study of dreams represents a crucial intersection between philosophical, psychological, neuroscientific, and clinical interests. Importantly, one of the main sources of insight into dreaming activity are the (oral or written) reports provided by dreamers upon awakening from their sleep. Classically, two main types of information are commonly extracted from dream reports: structural and semantic, content-related information. Extracted structural information is typically limited to the simple count of words or sentences in a report. Instead, content analysis usually relies on quantitative scores assigned by two or more (blind) human operators through the use of predefined coding systems. Within this review, we will show that methods borrowed from the field of linguistic analysis, such as graph analysis, dictionary-based content analysis, and distributional semantics approaches, could be used to complement and, in many cases, replace classical measures and scales for the quantitative structural and semantic assessment of dream reports. Importantly, these methods allow the direct (operator-independent) extraction of quantitative information from language data, hence enabling a fully objective and reproducible analysis of conscious experiences occurring during human sleep. Most importantly, these approaches can be partially or fully automatized and may thus be easily applied to the analysis of large datasets.},
  AUTHOR = {Elce, Valentina and Handjaras, Giacomo and Bernardi, Giulio},
  URL = {https://www.mdpi.com/2624-5175/3/3/35},
  DATE = {2021},
  DOI = {10.3390/clockssleep3030035},
  ISSN = {2624-5175},
  JOURNALTITLE = {Clocks \& Sleep},
  NOTE = {tex.pubmedid: 34563057},
  NUMBER = {3},
  PAGES = {495--514},
  TITLE = {The language of dreams: Application of linguistics-based approaches for the automated analysis of dream experiences},
  VOLUME = {3},
}

@INPROCEEDINGS{etienne-etal-2022-psycho,
  ABSTRACT = {This paper presents a scheme for emotion annotation and its manual application on a genre-diverse corpus of texts written in French. The methodology introduced here emphasizes the necessity of clarifying the main concepts implied by the analysis of emotions as they are expressed in texts, before conducting a manual annotation campaign. After explaining whatentails a deeply linguistic perspective on emotion expression modeling, we present a few {NLP} works that share some common points with this perspective and meticulously compare our approach with them. We then highlight some interesting quantitative results observed on our annotated corpus. The most notable interactions are on the one hand between emotion expression modes and genres of texts, and on the other hand between emotion expression modes and emotional categories. These observation corroborate and clarify some of the results already mentioned in other {NLP} works on emotion annotation.},
  AUTHOR = {Etienne, Aline and Battistelli, Delphine and Lecorvé, Gwénolé},
  LOCATION = {Marseille, France},
  PUBLISHER = {European Language Resources Association},
  URL = {https://aclanthology.org/2022.lrec-1.64},
  BOOKTITLE = {Proceedings of the thirteenth language resources and evaluation conference},
  DATE = {2022-06},
  PAGES = {603--612},
  TITLE = {A (psycho-)linguistically motivated scheme for annotating and exploring emotions in a genre-diverse corpus},
}

@ARTICLE{ferraraDetectionPromotedSocial2016,
  ABSTRACT = {Information spreading on social media contributes to the formation of collective opinions. Millions of social media users are exposed every day to popular memes — some generated organically by grassroots activity, others sustained by advertising, information campaigns or more or less transparent coordinated efforts. While most information campaigns are benign, some may have nefarious purposes, including terrorist propaganda, political astroturf, and financial market manipulation. This poses a crucial technological challenge with deep social implications: can we detect whether the spreading of a viral meme is being sustained by a promotional campaign? Here we study trending memes that attract attention either organically, or by means of advertisement. We designed a machine learning framework capable to detect promoted campaigns and separate them from organic ones in their early stages. Using a dataset of millions of posts associated with trending Twitter hashtags, we prove that remarkably accurate early detection is possible, achieving 95\% AUC score. Feature selection analysis reveals that network diffusion patterns and content cues are powerful early detection signals.},
  AUTHOR = {Ferrara, Emilio and Varol, Onur and Menczer, Filippo and Flammini, Alessandro},
  DATE = {2016},
  FILE = {C:\Users\Gustave\Zotero\storage\LIJZRLWC\Ferrara et al. - 2016 - Detection of Promoted Social Media Campaigns.pdf},
  ISSN = {2334-0770},
  JOURNALTITLE = {Proceedings of the International AAAI Conference on Web and Social Media},
  LANGID = {english},
  NUMBER = {1},
  PAGES = {563--566},
  TITLE = {Detection of Promoted Social Media Campaigns},
  VOLUME = {10},
}

@BOOK{Finkel2022,
  AUTHOR = {Finkel, Alain},
  LOCATION = {Paris},
  PUBLISHER = {Dunod},
  DATE = {2022},
  TITLE = {Manuel d'analyse cognitive des émotions: Théorie et applications},
}

@BOOK{fisherDesignExperiments1935,
  ABSTRACT = {Different types of experimentation are considered with reference to their logical structure, to show that valid conclusions may be drawn from them without using the disputed theory of inductive inferences, i.e., of arguing from observation to explanatory theory. This is possible if a null hypothesis is explicitly formulated when the experiment is designed; this hypothesis can never be proved, but may be disproved with whatever probability one will accept as demonstrating a positive result. Chapters II, III, and IV illustrate simple applications of the principles involved in sensitiveness, significance, tests of wider hypotheses, validity, and estimation and elimination of error. More elaborate structures are treated in later chapters. Chapter titles are: (V) the Latin square; (VI) factorial design in experimentation; (VII) confounding; (VIII) special cases of partial confounding; (IX) increase of precision by concomitant measurements: statistical control; (X) generalization of null hypotheses: fiducial probability; (XI) measurement of amount of information in general. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  AUTHOR = {Fisher, R. A.},
  LOCATION = {Oxford, England},
  PUBLISHER = {Oliver \& Boyd},
  DATE = {1935},
  SERIES = {The Design of Experiments},
  TITLE = {The Design of Experiments},
}

@ARTICLE{hvdc,
  AUTHOR = {Flanagan, H. M.},
  DATE = {1966},
  DOI = {10.1192/bjp.112.490.963},
  JOURNALTITLE = {The British Journal of Psychiatry},
  NOTE = {Publisher: Cambridge University Press},
  NUMBER = {490},
  PAGES = {963--964},
  TITLE = {The content analysis of dreams. By calvin S. Hall and robert L. Van de castle new york: The century psychology series. 1966. Pp. 320. Price not given.},
  VOLUME = {112},
}

@ARTICLE{ourdreams,
  AUTHOR = {Fogli, Alessandro and Aiello, Luca Maria and Quercia, Daniele},
  URL = {https://doi.org/10.1098/rsos.192080},
  DATE = {2020-08},
  DOI = {10.1098/rsos.192080},
  JOURNALTITLE = {Royal Society Open Science},
  NOTE = {Publisher: The Royal Society},
  NUMBER = {8},
  PAGES = {192080},
  TITLE = {Our dreams, our selves: automatic analysis of dream reports},
  VOLUME = {7},
}

@INCOLLECTION{ball_conversational_2024,
  AUTHOR = {Foppolo, Francesca and Mazzaggio, Greta},
  EDITOR = {Ball, Martin J. and Müller, Nicole and Spencer, Elizabeth},
  LANGUAGE = {en},
  PUBLISHER = {Wiley},
  URL = {https://onlinelibrary.wiley.com/doi/10.1002/9781119875949.ch2},
  BOOKTITLE = {The {Handbook} of {Clinical} {Linguistics}, {Second} {Edition}},
  DATE = {2024-01},
  DOI = {10.1002/9781119875949.ch2},
  EDITION = {1},
  ISBN = {978-1-119-87590-1 978-1-119-87594-9},
  PAGES = {15--27},
  TITLE = {Conversational {Implicature} and {Communication} {Disorders}},
  URLDATE = {2024-02-13},
}

@INCOLLECTION{freud,
  AUTHOR = {Freud, Sigmund},
  PUBLISHER = {Columbia University Press},
  BOOKTITLE = {Literature and psychoanalysis},
  DATE = {1983},
  PAGES = {29--33},
  TITLE = {The interpretation of dreams},
}

@MISC{gandhiUnderstandingSocialReasoning2023,
  ABSTRACT = {As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.},
  AUTHOR = {Gandhi, Kanishk and Fränken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah D.},
  ORGANIZATION = {arXiv},
  DATE = {2023-12},
  DOI = {10.48550/arXiv.2306.15448},
  EPRINT = {2306.15448},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  TITLE = {Understanding {{Social Reasoning}} in {{Language Models}} with {{Language Models}}},
}

@BOOK{gibbonsForensicLinguisticsIntroduction2003,
  ABSTRACT = {Forensic Linguistics is an introduction to the fascinating interface between language and the law. Provides an integrated and fully theorized understanding of language and law issues. Contains many helpful examples from genuine legal contexts and texts. Discusses linguistic sources of disadvantage before the law, particularly for ethnic minorities, children and abused women.},
  AUTHOR = {Gibbons, John},
  PUBLISHER = {Wiley},
  DATE = {2003},
  ISBN = {978-0-631-21247-8},
  KEYWORDS = {Language Arts \textbackslash\& Disciplines / General,Language Arts \textbackslash\& Disciplines / Linguistics / General},
  LANGID = {english},
  SHORTTITLE = {Forensic Linguistics},
  TITLE = {Forensic Linguistics: {{An}} Introduction to Language in the Justice System},
}

@INPROCEEDINGS{gildea-jurafsky-2000-automatic,
  AUTHOR = {Gildea, Daniel and Jurafsky, Daniel},
  LOCATION = {Hong Kong},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/P00-1065},
  BOOKTITLE = {Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics},
  DATE = {2000-10},
  DOI = {10.3115/1075218.1075283},
  PAGES = {512--520},
  TITLE = {Automatic Labeling of Semantic Roles},
}

@BOOK{granger1968essai,
  AUTHOR = {Granger, Gilles-Gaston},
  LOCATION = {Paris},
  PUBLISHER = {A. Colin},
  DATE = {1968},
  TITLE = {Essai d'une Philosophie Du Style},
}

@MISC{grattafioriLlama3Herd2024,
  ABSTRACT = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  AUTHOR = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and {Guangyi} and {Zhang} and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and {Yu} and {Wang} and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
  ORGANIZATION = {arXiv},
  DATE = {2024},
  KEYWORDS = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  TITLE = {The Llama 3 Herd of Models},
}

@BOOK{green_self-expression_2007,
  AUTHOR = {Green, Mitchell},
  LOCATION = {Oxford},
  PUBLISHER = {Oxford University Press},
  DATE = {2007},
  FILE = {Green 2007 Self-expression.pdf:/Users/bonard/switchdrive2/bibliothèque/Green 2007 Self-expression.pdf:application/pdf},
  TITLE = {Self-expression},
}

@ARTICLE{grice_meaning_1957,
  AUTHOR = {Grice, H. Paul},
  DATE = {1957},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/ZP4KI6S6/Grice - 1957 - Meaning.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/YEPH8GB7/2182440.html:text/html},
  JOURNALTITLE = {The Philosophical Review},
  NUMBER = {3},
  PAGES = {377--388},
  TITLE = {Meaning},
  VOLUME = {66},
}

@INCOLLECTION{grice_logic_1975,
  AUTHOR = {Grice, H. Paul},
  LOCATION = {Leiden},
  PUBLISHER = {Brill},
  BOOKTITLE = {Speech acts},
  DATE = {1975},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/EQYUWPNX/Grice - 1975 - Logic and conversation.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/VDH9XU4T/BP000003.html:text/html},
  PAGES = {41--58},
  TITLE = {Logic and conversation},
}

@BOOK{grice_studies_1989,
  AUTHOR = {Grice, H. Paul},
  LOCATION = {Cambridge (MA)},
  PUBLISHER = {Harvard University Press},
  DATE = {1989},
  FILE = {Grice 1989 Studies_in_the_Way_of_Words__.pdf:/Users/bonard/switchdrive2/bibliothèque/Grice 1989 Studies in-the-way-of-words.djvu:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/ZW9JTGBZ/books.html:text/html},
  TITLE = {Studies in the way of words},
}

@MISC{gurRealWorldWebAgentPlanning2023,
  ABSTRACT = {Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50\%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7\% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.},
  AUTHOR = {Gur, Izzeddin and Furuta, Hiroki and Huang, Austin and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
  ORGANIZATION = {arXiv},
  DATE = {2023-10},
  DOI = {10.48550/arXiv.2307.12856},
  EPRINT = {2307.12856},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  TITLE = {A {{Real-World WebAgent}} with {{Planning}}, {{Long Context Understanding}}, and {{Program Synthesis}}},
}

@ARTICLE{GUTMANMUSIC2022103428,
  ABSTRACT = {This article demonstrates that an automated system of linguistic analysis can be developed – the Oneirograph – to analyze large collections of dreams and computationally map their contents in terms of typical situations involving an interplay of characters, activities, and settings. Focusing the analysis first on the twin situations of fighting and fleeing, the results provide densely detailed empirical evidence of the underlying semantic structures of typical dreams. The results also indicate that the Oneirograph analytic system can be applied to other typical dream situations as well (e.g., flying, falling), each of which can be computationally mapped in terms of a distinctive constellation of characters, activities, and settings.},
  AUTHOR = {Gutman Music, Maja and Holur, Pavan and Bulkeley, Kelly},
  URL = {https://www.sciencedirect.com/science/article/pii/S105381002200160X},
  DATE = {2022},
  DOI = {https://doi.org/10.1016/j.concog.2022.103428},
  ISSN = {1053-8100},
  JOURNALTITLE = {Consciousness and Cognition},
  KEYWORDS = {Dreams,Fight/Flight,Flying/Falling,Natural Language Processing ({NLP}),Network analysis,Pattern quantification,Representation learning,Semantic structure,Typical situation},
  PAGES = {103428},
  TITLE = {Mapping dreams in a computational space: A phrase-level model for analyzing Fight/Flight and other typical situations in dream reports},
  VOLUME = {106},
}

@BOOK{hadamard1945essay,
  AUTHOR = {Hadamard, Jacques},
  LOCATION = {Princeton, N.J.},
  PUBLISHER = {Princeton University Press},
  URL = {https://books.google.com/books?id=W2U4AAAAMAAJ},
  DATE = {1945},
  TITLE = {An Essay on the Psychology of Invention in the Mathematical Field},
}

@BOOK{hallidayIntroductionFunctionalGrammar2014,
  AUTHOR = {Halliday, M.A.K. and Matthiessen, Christian M.I.M. and Halliday, Michael and Matthiessen, Christian},
  PUBLISHER = {Routledge},
  DATE = {2014},
  ISBN = {978-1-4441-1908-4},
  LANGID = {english},
  TITLE = {An Introduction to Functional Grammar},
}

@BOOK{oneiro,
  AUTHOR = {Harris-{McCoy}, Daniel E.},
  PUBLISHER = {Oxford University Press},
  DATE = {2012},
  TITLE = {Artemidorus' oneirocritica: Text, translation, and commentary},
}

@BOOK{heim_semantics_1998,
  ABSTRACT = {Written by two of the leading figures in the field, this is a lucid and systematic introduction to formal semantics.},
  AUTHOR = {Heim, Irene and Kratzer, Angelika},
  LANGUAGE = {en},
  LOCATION = {Hoboken},
  PUBLISHER = {Wiley},
  DATE = {1998-01},
  ISBN = {978-0-631-19713-3},
  KEYWORDS = {Language Arts \& Disciplines / Linguistics / General,Language Arts \& Disciplines / General},
  NOTE = {Google-Books-ID: jAvR2DB3pPIC},
  TITLE = {Semantics in generative grammar},
}

@ARTICLE{heintz_expression_2023,
  ABSTRACT = {Human expression is open-ended, versatile and diverse, ranging from ordinary language use to painting, from exaggerated displays of affection to micro-movements that aid coordination. Here we present and defend the claim that this expressive diversity is united by an interrelated suite of cognitive capacities, the evolved functions of which are the expression and recognition of informative intentions. We describe how evolutionary dynamics normally leash communication to narrow domains of statistical mutual benefit, and how they are unleashed in humans. The relevant cognitive capacities are cognitive adaptations to living in a partner choice social ecology; and they are, correspondingly, part of the ordinarily developing human cognitive phenotype, emerging early and reliably in ontogeny. In other words, we identify distinctive features of our species‘ social ecology to explain how and why humans, and only humans, evolved the cognitive capacities that, in turn, lead to massive diversity and open-endedness in means and modes of expression. Language use is but one of these modes of expression, albeit one of manifestly high importance. We make cross-species comparisons, describe how the relevant cognitive capacities can evolve in a gradual manner, and survey how unleashed expression facilitates not only language use but novel behaviour in many other domains too, focusing on the examples of joint action, teaching, punishment and art, all of which are ubiquitous in human societies but relatively rare in other species. Much of this diversity derives from graded aspects of human expression, which can be used to satisfy informative intentions in creative and new ways. We aim to help reorient cognitive pragmatics, as a phenomenon that is not a supplement to linguistic communication and on the periphery of language science, but rather the foundation of the many of the most distinctive features of human behaviour, society and culture.},
  AUTHOR = {Heintz, Christophe and Scott-Phillips, Thom},
  LANGUAGE = {en-us},
  DATE = {2023},
  DOI = {10.31234/osf.io/mcv5b},
  FILE = {Full Text PDF:/Users/bonard/switchdrive2/Zotero/storage/YD42HVTC/Heintz et Scott-Phillips - 2021 - Expression Unleashed The evolutionary & cognitive.pdf:application/pdf},
  JOURNALTITLE = {Behavioral and Brain Sciences},
  KEYWORDS = {pragmatics,language,communication,language evolution,Linguistics,Psychology,Animal Learning and Behavior,art,cognition,cognitive anthropology,Cognitive Development,Cognitive Psychology,comparative cognition,Developmental Psychology,Evolution,evolutionary psychology,joint action,Language,Life Sciences,other,partner choice,philosophy of language,Psycholinguistics and Neurolinguistics,punishment,Semantics and Pragmatics,Social and Behavioral Sciences,social minds},
  NOTE = {type: article},
  PAGES = {E1},
  SHORTTITLE = {Expression {Unleashed}},
  TITLE = {Expression unleashed: {The} evolutionary \& cognitive foundations of human communication},
  URLDATE = {2022-03-07},
  VOLUME = {46},
}

@ARTICLE{hipolitoPatternBreakingComplex2023a,
  ABSTRACT = {Abstract Recent research has demonstrated the potential of psychedelic therapy for mental health care. However, the psychological experience underlying its therapeutic effects remains poorly understood. This paper proposes a framework that suggests psychedelics act as destabilizers, both psychologically and neurophysiologically. Drawing on the ‘entropic brain’ hypothesis and the ‘RElaxed Beliefs Under pSychedelics’ model, this paper focuses on the richness of psychological experience. Through a complex systems theory perspective, we suggest that psychedelics destabilize fixed points or attractors, breaking reinforced patterns of thinking and behaving. Our approach explains how psychedelic-induced increases in brain entropy destabilize neurophysiological set points and lead to new conceptualizations of psychedelic psychotherapy. These insights have important implications for risk mitigation and treatment optimization in psychedelic medicine, both during the peak psychedelic experience and during the subacute period of potential recovery.},
  AUTHOR = {Hipólito, Inês and Mago, Jonas and Rosas, Fernando E and Carhart-Harris, Robin},
  DATE = {2023},
  ISSN = {2057-2107},
  JOURNALTITLE = {Neuroscience of Consciousness},
  LANGID = {english},
  NUMBER = {1},
  PAGES = {niad017},
  SHORTTITLE = {Pattern Breaking},
  TITLE = {Pattern Breaking: A Complex Systems Approach to Psychedelic Medicine},
  VOLUME = {2023},
}

@INPROCEEDINGS{hofmann-etal-2020-appraisal,
  AUTHOR = {Hofmann, Jan and Troiano, Enrica and Sassenberg, Kai and Klinger, Roman},
  LOCATION = {Barcelona, Spain (Online)},
  PUBLISHER = {International Committee on Computational Linguistics},
  URL = {https://aclanthology.org/2020.coling-main.11},
  BOOKTITLE = {Proceedings of the 28th international conference on computational linguistics},
  DATE = {2020-12},
  DOI = {10.18653/v1/2020.coling-main.11},
  PAGES = {125--138},
  TITLE = {Appraisal theories for emotion classification in text},
}

@MISC{holterman_does_2023,
  ABSTRACT = {Theory of Mind (ToM) is the ability to understand human thinking and decision-making, an ability that plays a crucial role in social interaction between people, including linguistic communication. This paper investigates to what extent recent Large Language Models in the ChatGPT tradition possess ToM. We posed six well-known problems that address biases in human reasoning and decision making to two versions of ChatGPT and we compared the results under a range of prompting strategies. While the results concerning ChatGPT-3 were somewhat inconclusive, ChatGPT-4 was shown to arrive at the correct answers more often than would be expected based on chance, although correct answers were often arrived at on the basis of false assumptions or invalid reasoning.},
  AUTHOR = {Holterman, Bart and van Deemter, Kees},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/2305.14020},
  DATE = {2023-09},
  FILE = {arXiv Fulltext PDF:/Users/bonard/switchdrive2/Zotero/storage/F4NPXQTN/Holterman et van Deemter - 2023 - Does ChatGPT have Theory of Mind.pdf:application/pdf},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  NOTE = {arXiv:2305.14020 [cs]},
  TITLE = {Does {ChatGPT} have {Theory} of {Mind}?},
  URLDATE = {2023-12-08},
}

@ARTICLE{hongLexicalUseEmotional2015,
  ABSTRACT = {Language dysfunction has long been described in schizophrenia and most studies have focused on characteristics of structure and form. This project focuses on the content of language based on autobiographical narratives of five basic emotions. In persons with schizophrenia and healthy controls, we employed a comprehensive automated analysis of lexical use and we identified specific words and semantically or functionally related words derived from dictionaries that occurred significantly more often in narratives of either group. Patients employed a similar number of words but differed in lower expressivity and complexity, more self-reference and more repetitions. We developed a classification method for predicting subject status and tested its accuracy in a leave-one-subject-out evaluation procedure. We identified a set of 18 features that achieved 65.7\% accuracy in predicting clinical status based on single emotion narratives, and 74.4\% accuracy based on all five narratives. Subject clinical status could be determined automatically more accurately based on narratives related to anger or happiness experiences and there were a larger number of lexical differences between the two groups for these emotions compared to other emotions.},
  AUTHOR = {Hong, Kai and Nenkova, Ani and March, Mary E. and Parker, Amber P. and Verma, Ragini and Kohler, Christian G.},
  DATE = {2015},
  ISSN = {0165-1781},
  JOURNALTITLE = {Psychiatry Research},
  KEYWORDS = {Diction,Emotion,Learning-based analyses,Lexical features,LIWC,Text classification},
  NUMBER = {1},
  PAGES = {40--49},
  TITLE = {Lexical Use in Emotional Autobiographical Narratives of Persons with Schizophrenia and Healthy Controls},
  VOLUME = {225},
}

@INPROCEEDINGS{honnibal2010handoff,
  AUTHOR = {Honnibal, Matthew and Curran, James R.},
  LOCATION = {Prague, Czech Republic},
  PUBLISHER = ACL,
  URL = {https://www.cs.brandeis.edu/~marc/misc/proceedings/acl-2007/ACL2007_DLP/14.pdf},
  BOOKTITLE = {Proceedings of the {{ACL}} 2007 Workshop on Deep Linguistic Processing},
  DATE = {2007},
  PAGES = {89--96},
  TITLE = {Creating a Systemic Functional Grammar Corpus from the {{Penn}} Treebank},
}

@UNPUBLISHED{spacy2,
  AUTHOR = {Honnibal, Matthew and Montani, Ines},
  DATE = {2017},
  TITLE = {{spaCy} 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing},
}

@BOOK{husserl2012ideas,
  AUTHOR = {Husserl, Edmund},
  LOCATION = {Dordrecht},
  PUBLISHER = {Springer Science \& Business Media},
  URL = {https://books.google.com/books/about/Ideas_Pertaining_to_a_Pure_Phenomenology.html?id=geEsBAAAQBAJ},
  DATE = {2012},
  EDITION = {illustrated},
  ISBN = {978-94-009-7445-6},
  TITLE = {Ideas Pertaining to a Pure Phenomenology and to a Phenomenological Philosophy: {{First}} Book: {{General}} Introduction to a Pure Phenomenology},
}

@ARTICLE{izard_basic_1992,
  AUTHOR = {Izard, Carroll E.},
  URL = {http://www.communicationcache.com/uploads/1/0/8/8/10887248/basic_emotions_relations_among_emotions_and_emotion-cognition_relations.pdf},
  DATE = {1992},
  FILE = {Available Version (via Google Scholar):/Users/bonard/switchdrive2/Zotero/storage/C29FI239/Izard - 1992 - Basic Emotions, Relations Among Emotions, and Emot.pdf:application/pdf},
  JOURNALTITLE = {Psychological Review},
  NUMBER = {3},
  PAGES = {561--565},
  TITLE = {Basic {Emotions}, {Relations} {Among} {Emotions}, and {Emotion}-{Cognition} {Relations}},
  URLDATE = {2024-02-12},
  VOLUME = {99},
}

@INCOLLECTION{jakobsonLinguisticsPoetics2010,
  AUTHOR = {Jakobson, Roman},
  PUBLISHER = {De Gruyter Mouton},
  BOOKTITLE = {Volume {{III}} Poetry of Grammar and Grammar of Poetry},
  DATE = {2010},
  ISBN = {978-3-11-080212-2},
  LANGID = {english},
  PAGES = {18--51},
  TITLE = {Linguistics and Poetics},
}

@ARTICLE{jinDeepLearningText2022,
  ABSTRACT = {Text style transfer is an important task in natural language generation, which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing, and recently has re-gained significant attention thanks to the promising performance brought by deep neural models. In this article, we present a systematic survey of the research on neural text style transfer, spanning over 100 representative articles since the first neural text style transfer work in 2017. We discuss the task formulation, existing datasets and subtasks, evaluation, as well as the rich methodologies in the presence of parallel and non-parallel data. We also provide discussions on a variety of important topics regarding the future development of this task.1},
  AUTHOR = {Jin, Di and Jin, Zhijing and Hu, Zhiting and Vechtomova, Olga and Mihalcea, Rada},
  DATE = {2022},
  FILE = {C\:\\Users\\Gustave\\Zotero\\storage\\YNV549B5\\Jin et al. - 2022 - Deep Learning for Text Style Transfer A Survey.pdf;C\:\\Users\\Gustave\\Zotero\\storage\\U8M538FP\\Deep-Learning-for-Text-Style-Transfer-A-Survey.html},
  ISSN = {0891-2017},
  JOURNALTITLE = {Computational Linguistics},
  NUMBER = {1},
  PAGES = {155--205},
  SHORTTITLE = {Deep Learning for Text Style Transfer},
  TITLE = {Deep Learning for Text Style Transfer: A Survey},
  VOLUME = {48},
}

@INPROCEEDINGS{kimWhoFeelsWhat2018,
  ABSTRACT = {Most approaches to emotion analysis in fictional texts focus on detecting the emotion expressed in text. We argue that this is a simplification which leads to an overgeneralized interpretation of the results, as it does not take into account who experiences an emotion and why. Emotions play a crucial role in the interaction between characters and the events they are involved in. Until today, no specific corpora that capture such an interaction were available for literature. We aim at filling this gap and present a publicly available corpus based on Project Gutenberg, REMAN (Relational EMotion ANnotation), manually annotated for spans which correspond to emotion trigger phrases and entities/events in the roles of experiencers, targets, and causes of the emotion. We provide baseline results for the automatic prediction of these relational structures and show that emotion lexicons are not able to encompass the high variability of emotion expressions and demonstrate that statistical models benefit from joint modeling of emotions with its roles in all subtasks. The corpus that we provide enables future research on the recognition of emotions and associated entities in text. It supports qualitative literary studies and digital humanities. The corpus is available at http://www.ims.uni-stuttgart.de/data/reman .},
  AUTHOR = {Kim, Evgeny and Klinger, Roman},
  LOCATION = {Santa Fe, New Mexico, USA},
  PUBLISHER = ACL,
  BOOKTITLE = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  DATE = {2018-08},
  KEYWORDS = {notion},
  PAGES = {1345--1359},
  SHORTTITLE = {Who {{Feels What}} and {{Why}}?},
  TITLE = {Who {{Feels What}} and {{Why}}? {{Annotation}} of a {{Literature Corpus}} with {{Semantic Roles}} of {{Emotions}}},
}

@INPROCEEDINGS{reman,
  AUTHOR = {Kim, Evgeny and Klinger, Roman},
  LOCATION = {Santa Fe, {USA}},
  BOOKTITLE = {Proceedings of {COLING} 2018, the 27th international conference on computational linguistics},
  DATE = {2018-08},
  TITLE = {Who feels what and why? Annotation of a literature corpus with semantic roles of emotions},
}

@INPROCEEDINGS{kim-klinger-2019-analysis,
  AUTHOR = {Kim, Evgeny and Klinger, Roman},
  LOCATION = {Florence, Italy},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/W19-3406},
  BOOKTITLE = {Proceedings of the Second Workshop on Storytelling},
  DATE = {2019-08},
  DOI = {10.18653/v1/W19-3406},
  PAGES = {56--64},
  TITLE = {An Analysis of Emotion Communication Channels in Fan-Fiction: Towards Emotional Storytelling},
}

@MISC{kimFANToMBenchmarkStresstesting2023,
  ABSTRACT = {Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.},
  AUTHOR = {Kim, Hyunwoo and Sclar, Melanie and Zhou, Xuhui and Bras, Ronan Le and Kim, Gunhee and Choi, Yejin and Sap, Maarten},
  ORGANIZATION = {arXiv},
  DATE = {2023-10},
  DOI = {10.48550/arXiv.2310.15421},
  EPRINT = {2310.15421},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  SHORTTITLE = {{{FANToM}}},
  TITLE = {{{FANToM}}: {{A Benchmark}} for {{Stress-testing Machine Theory}} of {{Mind}} in {{Interactions}}},
}

@INPROCEEDINGS{klingerWhereAreWe2023,
  ABSTRACT = {The term emotion analysis in text subsumes various natural language processing tasks which have in common the goal to enable computers to understand emotions. Most popular is emotion classification in which one or multiple emotions are assigned to a predefined textual unit. While such setting is appropriate for identifying the reader's or author's emotion, emotion role labeling adds the perspective of mentioned entities and extracts text spans that correspond to the emotion cause. The underlying emotion theories agree on one important point; that an emotion is caused by some internal or external event and comprises several subcomponents, including the subjective feeling and a cognitive evaluation. We therefore argue that emotions and events are related in two ways. (1) Emotions are events; and this perspective is the fundament in natural language processing for emotion role labeling. (2) Emotions are caused by events; a perspective that is made explicit with research how to incorporate psychological appraisal theories in NLP models to interpret events. These two research directions, role labeling and (event-focused) emotion classification, have by and large been tackled separately. In this paper, we contextualize both perspectives and discuss open research questions.},
  AUTHOR = {Klinger, Roman},
  EDITOR = {Elazar, Yanai and Ettinger, Allyson and Kassner, Nora and Ruder, Sebastian and A. Smith, Noah},
  LOCATION = {Singapore},
  PUBLISHER = ACL,
  BOOKTITLE = {Proceedings of the {{Big Picture Workshop}}},
  DATE = {2023-12},
  DOI = {10.18653/v1/2023.bigpicture-1.1},
  PAGES = {1--17},
  SHORTTITLE = {Where Are {{We}} in {{Event-centric Emotion Analysis}}?},
  TITLE = {Where Are {{We}} in {{Event-centric Emotion Analysis}}? {{Bridging Emotion Role Labeling}} and {{Appraisal-based Approaches}}},
}

@ARTICLE{knuthFastPatternMatching1977,
  ABSTRACT = {We present the correction to Knuth’s algorithm [2] for computing the table of pattern shifts later used in the Boyer–Moore algorithm for pattern matching.},
  AUTHOR = {Knuth, Donald E. and Morris, Jr., James H. and Pratt, Vaughan R.},
  URL = {https://epubs.siam.org/doi/10.1137/0206024},
  DATE = {1977-06},
  DOI = {10.1137/0206024},
  ISSN = {0097-5397},
  JOURNALTITLE = {SIAM Journal on Computing},
  NUMBER = {2},
  PAGES = {323--350},
  TITLE = {Fast Pattern Matching in Strings},
  URLDATE = {2025-08-01},
  VOLUME = {6},
}

@MISC{kojimaLargeLanguageModels2023,
  ABSTRACT = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  AUTHOR = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  ORGANIZATION = {arXiv},
  DATE = {2023-01},
  DOI = {10.48550/arXiv.2205.11916},
  EPRINT = {2205.11916},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  TITLE = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
}

@MISC{kosinski_theory_2023,
  ABSTRACT = {We explore the intriguing possibility that theory of mind (ToM), or the uniquely human ability to impute unobservable mental states to others, might have spontaneously emerged in large language models (LLMs). We designed 40 false-belief tasks, considered a gold standard in testing ToM in humans, and administered them to several LLMs. Each task included a false-belief scenario, three closely matched true-belief controls, and the reversed versions of all four. Smaller and older models solved no tasks; GPT-3-davinci-003 (from November 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20\% of the tasks; ChatGPT-4 (from June 2023) solved 75\% of the tasks, matching the performance of six-year-old children observed in past studies. These findings suggest the intriguing possibility that ToM, previously considered exclusive to humans, may have spontaneously emerged as a byproduct of LLMs' improving language skills.},
  AUTHOR = {Kosinski, Michal},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/2302.02083},
  DATE = {2023-11},
  DOI = {10.48550/arXiv.2302.02083},
  FILE = {arXiv Fulltext PDF:/Users/bonard/switchdrive2/Zotero/storage/HL9CGAT9/Kosinski - 2023 - Theory of Mind Might Have Spontaneously Emerged in.pdf:application/pdf},
  KEYWORDS = {Computer Science - Computers and Society,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  NOTE = {arXiv:2302.02083 [cs]},
  TITLE = {Theory of {Mind} {Might} {Have} {Spontaneously} {Emerged} in {Large} {Language} {Models}},
  URLDATE = {2023-12-08},
}

@ARTICLE{labovNarrativeAnalysisOral1997,
  ABSTRACT = {Presents an analytical framework for the analysis of oral versions of personal experience in English. The subject matter comes from tape-recorded narratives taken from 2 distinct social contexts. One is a face-to-face interview where the narrator is speaking only to the interviewer. In the second situation, the narrator is recorded in interaction with his primary group. 14 examples of narratives drawn from about 600 interviews gathered in the course of 4 linguistic studies are given. The narrators include speakers from Black and White communities and rural and urban areas. Narrators range in age from 10–72 yrs old, and none finished high school. Definitions of the basic units of narrative are presented, and the normal structure of the narrative as a whole is outlined. Some general propositions about the relation of formal properties to narrative functions are presented. Since the authors are concerned with problems of effective communication and class and ethnic differences in verbal behavior, close correlations of the narrator's social characteristics with the structure of their narratives is required. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  AUTHOR = {Labov, William and Waletzky, Joshua},
  DATE = {1997},
  FILE = {C:\Users\Gustave\Zotero\storage\7XSYX4X6\1997-39195-001.html},
  ISSN = {1053-6981},
  JOURNALTITLE = {Journal of Narrative \& Life History},
  KEYWORDS = {Discourse Analysis,Life Experiences,Narrative Analysis,Narratives,Oral Communication,Thematic Analysis},
  NUMBER = {1},
  PAGES = {3--38},
  SHORTTITLE = {Narrative Analysis},
  TITLE = {Narrative Analysis: {{Oral}} Versions of Personal Experience},
  VOLUME = {7},
}

@BOOK{sociologiereve,
  AUTHOR = {Lahire, B.},
  PUBLISHER = {La Découverte},
  URL = {https://books.google.fr/books?id=xzQNEAAAQBAJ},
  DATE = {2021},
  ISBN = {978-2-348-06750-1},
  SERIES = {Poche / Sciences humaines et sociales},
  TITLE = {L'interprétation sociologique des rêves},
}

@ARTICLE{langley_theory_2022,
  ABSTRACT = {Theory of Mind (ToM)—the ability of the human mind to attribute mental states to others—is a key component of human cognition. In order to understand other people's mental states or viewpoint and to have successful interactions with others within social and occupational environments, this form of social cognition is essential. The same capability of inferring human mental states is a prerequisite for artificial intelligence (AI) to be integrated into society, for example in healthcare and the motoring industry. Autonomous cars will need to be able to infer the mental states of human drivers and pedestrians to predict their behavior. In the literature, there has been an increasing understanding of ToM, specifically with increasing cognitive science studies in children and in individuals with Autism Spectrum Disorder. Similarly, with neuroimaging studies there is now a better understanding of the neural mechanisms that underlie ToM. In addition, new AI algorithms for inferring human mental states have been proposed with more complex applications and better generalisability. In this review, we synthesize the existing understanding of ToM in cognitive and neurosciences and the AI computational models that have been proposed. We focus on preference learning as an area of particular interest and the most recent neurocognitive and computational ToM models. We also discuss the limitations of existing models and hint at potential approaches to allow ToM models to fully express the complexity of the human mind in all its aspects, including values and preferences.},
  AUTHOR = {Langley, Christelle and Cirstea, Bogdan Ionut and Cuzzolin, Fabio and Sahakian, Barbara J.},
  URL = {https://www.frontiersin.org/articles/10.3389/frai.2022.778852},
  DATE = {2022},
  FILE = {Langley et al 2023 Theory of Mind and Preference Learning at the Interface of Cognitive Science, Neuroscience, and AI – A Review.pdf:/Users/bonard/switchdrive2/academia/bibliothèque académique/Langley et al 2023 Theory of Mind and Preference Learning at the Interface of Cognitive Science, Neuroscience, and AI – A Review.pdf:application/pdf},
  ISSN = {2624-8212},
  JOURNALTITLE = {Frontiers in Artificial Intelligence},
  SHORTTITLE = {Theory of {Mind} and {Preference} {Learning} at the {Interface} of {Cognitive} {Science}, {Neuroscience}, and {AI}},
  TITLE = {Theory of {Mind} and {Preference} {Learning} at the {Interface} of {Cognitive} {Science}, {Neuroscience}, and {AI}: {A} {Review}},
  URLDATE = {2023-12-08},
  VOLUME = {5},
}

@ARTICLE{lazarus_progress_1991,
  AUTHOR = {Lazarus, Richard S.},
  DATE = {1991},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/FJZ759EF/1991-32296-001.html:text/html},
  JOURNALTITLE = {American psychologist},
  NUMBER = {8},
  PAGES = {819},
  TITLE = {Progress on a cognitive-motivational-relational theory of emotion.},
  VOLUME = {46},
}

@BOOK{lazarus,
  AUTHOR = {Lazarus, Richard S. and Folkman, Susan.},
  PUBLISHER = {Springer Pub. Co New York},
  DATE = {1984},
  ISBN = {0-8261-4190-0 0-8261-4191-9},
  NOTE = {Pages: xiii, 445 p. : Type: Book tex.life-dates: 1984 - tex.subjects: Stress (Psychology); Stress, Psychological.},
  TITLE = {Stress, appraisal, and coping},
}

@ARTICLE{ledgerShakespeareFletcherTwo1994,
  ABSTRACT = {This article shows how letter frequency measurements provide the means to discriminate between authors. The data is analysed using Cluster Analysis and other techniques of rhultivariate analysis. Texts of Shakespeare and Fletcher (and other authors) are found to differ markedly from each other. This information is used to ascertain which parts of Two Noble Kinsmen resemble Fletcher, and which resemble Shakespeare. The twenty-six scenes of the play are then allocated according to the results obtained.},
  AUTHOR = {Ledger, Gerard and Merriam, Thomas},
  DATE = {1994},
  FILE = {C:\Users\Gustave\Zotero\storage\N2THK9TZ\966575.html},
  ISSN = {0268-1145},
  JOURNALTITLE = {Literary and Linguistic Computing},
  NUMBER = {3},
  PAGES = {235--248},
  TITLE = {Shakespeare, Fletcher, and the Two Noble Kinsmen},
  VOLUME = {9},
}

@INPROCEEDINGS{lee-etal-2010-text,
  AUTHOR = {Lee, Sophia Yat Mei and Chen, Ying and Huang, Chu-Ren},
  LOCATION = {Los Angeles, CA},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/W10-0206},
  BOOKTITLE = WS:2010:2,
  DATE = {2010-06},
  PAGES = {45--53},
  TITLE = {A Text-driven Rule-based System for Emotion Cause Detection},
}

@ARTICLE{lempelComplexityFiniteSequences1976,
  ABSTRACT = {A new approach to the problem of evaluating the complexity ("randomness") of finite sequences is presented. The proposed complexity measure is related to the number of steps in a self-delimiting production process by which a given sequence is presumed to be generated. It is further related to the number of distinct substrings and the rate of their occurrence along the sequence. The derived properties of the proposed measure are discussed and motivated in conjunction with other well-established complexity criteria.},
  AUTHOR = {Lempel, A. and Ziv, J.},
  DATE = {1976},
  ISSN = {1557-9654},
  JOURNALTITLE = {IEEE Transactions on Information Theory},
  NUMBER = {1},
  PAGES = {75--81},
  TITLE = {On the Complexity of Finite Sequences},
  VOLUME = {22},
}

@MISC{liangEncouragingDivergentThinking2023,
  ABSTRACT = {Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of "tit for tat" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of "tit for tat" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate},
  AUTHOR = {Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Tu, Zhaopeng and Shi, Shuming},
  ORGANIZATION = {arXiv},
  DATE = {2023-05},
  DOI = {10.48550/arXiv.2305.19118},
  EPRINT = {2305.19118},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Computation and Language},
  TITLE = {Encouraging {{Divergent Thinking}} in {{Large Language Models}} through {{Multi-Agent Debate}}},
}

@MISC{luLargeLanguageModels2024,
  ABSTRACT = {Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, in this study, we introduce Ditto, a self-alignment method for role-play. Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension. This method creates a role-play training set comprising 4,000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed and reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in various parameter scales, consistently maintains a consistent role identity and provides accurate role-specific knowledge in multi-turn role-play conversations. Notably, it outperforms all open-source role-play baselines, showcasing performance levels comparable to advanced proprietary chatbots. Furthermore, we present the first comprehensive cross-supervision alignment experiment in the role-play domain, revealing that the intrinsic capabilities of LLMs confine the knowledge within role-play. Meanwhile, the role-play styles can be easily acquired with the guidance of smaller models. We open-source related resources at https://github.com/OFA-Sys/Ditto.},
  AUTHOR = {Lu, Keming and Yu, Bowen and Zhou, Chang and Zhou, Jingren},
  ORGANIZATION = {arXiv},
  DATE = {2024-01},
  DOI = {10.48550/arXiv.2401.12474},
  EPRINT = {2401.12474},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  SHORTTITLE = {Large {{Language Models}} Are {{Superpositions}} of {{All Characters}}},
  TITLE = {Large {{Language Models}} Are {{Superpositions}} of {{All Characters}}: {{Attaining Arbitrary Role-play}} via {{Self-Alignment}}},
}

@MISC{maHolisticLandscapeSituated2023,
  ABSTRACT = {Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM. Project page: https://github.com/Mars-tin/awesome-theory-of-mind},
  AUTHOR = {Ma, Ziqiao and Sansom, Jacob and Peng, Run and Chai, Joyce},
  ORGANIZATION = {arXiv},
  DATE = {2023-10},
  DOI = {10.48550/arXiv.2310.19619},
  EPRINT = {2310.19619},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  TITLE = {Towards {{A Holistic Landscape}} of {{Situated Theory}} of {{Mind}} in {{Large Language Models}}},
}

@MISC{madaanSelfRefineIterativeRefinement2023,
  ABSTRACT = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by \textasciitilde 20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
  AUTHOR = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
  ORGANIZATION = {arXiv},
  DATE = {2023-05},
  DOI = {10.48550/arXiv.2303.17651},
  EPRINT = {2303.17651},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  SHORTTITLE = {Self-{{Refine}}},
  TITLE = {Self-{{Refine}}: {{Iterative Refinement}} with {{Self-Feedback}}},
}

@MISC{mahowaldDissociatingLanguageThought2023a,
  ABSTRACT = {Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence–knowledge of linguistic rules and patterns–and functional linguistic competence–understanding and using language in the world. We ground this distinction in human neuroscience, showing that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. In short, LLMs are good models of language but incomplete models of human thought.},
  AUTHOR = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  ORGANIZATION = {arXiv},
  DATE = {2023-11},
  DOI = {10.48550/arXiv.2301.06627},
  EPRINT = {2301.06627},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  TITLE = {Dissociating Language and Thought in Large Language Models},
}

@ARTICLE{Mallett2021,
  AUTHOR = {Mallett, Remington and Picard-Deland, Claudia and Pigeon, Wilfred and Wary, Madeline and Grewal, Alam and Blagrove, Mark and Carr, Michelle},
  URL = {https://doi.org/10.1007/s42761-021-00080-8},
  DATE = {2021-11},
  DOI = {10.1007/s42761-021-00080-8},
  JOURNALTITLE = {Affective Science},
  NOTE = {Publisher: Springer Science and Business Media {LLC}},
  NUMBER = {2},
  PAGES = {400--405},
  TITLE = {The relationship between dreams and subsequent morning mood using self-reports and text analysis},
  VOLUME = {3},
}

@INPROCEEDINGS{martinCamemBERTTastyFrench2020,
  ABSTRACT = {Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models –in all languages except English– very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+{GB}). Our best performing model {CamemBERT} reaches or improves the state of the art in all four downstream tasks.},
  AUTHOR = {Martin, Louis and Muller, Benjamin and Ortiz Suárez, Pedro Javier and Dupont, Yoann and Romary, Laurent and de la Clergerie, Éric and Seddah, Djamé and Sagot, Benoît},
  EDITOR = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  LOCATION = {Online},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2020.acl-main.645/},
  BOOKTITLE = ACL:2020:MAIN,
  DATE = {2020-07},
  DOI = {10.18653/v1/2020.acl-main.645},
  EVENTTITLE = {{ACL} 2020},
  FILE = {Full Text PDF:C\:\\Users\\Gustave\\Zotero\\storage\\4WWNJTJI\\Martin et al. - 2020 - CamemBERT a Tasty French Language Model.pdf:application/pdf},
  PAGES = {7203--7219},
  SHORTTITLE = {{CamemBERT}},
  TITLE = {{CamemBERT}: a Tasty French Language Model},
  URLDATE = {2025-09-24},
}

@ARTICLE{mayer_human_2008,
  ABSTRACT = {Emotional intelligence (EI) involves the ability to carry out accurate reasoning about emotions and the ability to use emotions and emotional knowledge to enhance thought. We discuss the origins of the EI concept, define EI, and describe the scope of the field today. We review three approaches taken to date from both a theoretical and methodological perspective. We find that Specific-Ability and Integrative-Model approaches adequately conceptualize and measure EI. Pivotal in this review are those studies that address the relation between EI measures and meaningful criteria including social outcomes, performance, and psychological and physical well-being. The Discussion section is followed by a list of summary points and recommended issues for future research.},
  AUTHOR = {Mayer, John D. and Roberts, Richard D. and Barsade, Sigal G.},
  LANGUAGE = {en},
  URL = {https://www.annualreviews.org/doi/10.1146/annurev.psych.59.103006.093646},
  DATE = {2008-01},
  DOI = {10.1146/annurev.psych.59.103006.093646},
  FILE = {Available Version (via Google Scholar):/Users/bonard/switchdrive2/Zotero/storage/QYYU8H9Y/Mayer et al. - 2008 - Human Abilities Emotional Intelligence.pdf:application/pdf},
  ISSN = {0066-4308, 1545-2085},
  JOURNALTITLE = {Annual Review of Psychology},
  NUMBER = {1},
  PAGES = {507--536},
  SHORTTITLE = {Human {Abilities}},
  TITLE = {Human {Abilities}: {Emotional} {Intelligence}},
  URLDATE = {2024-02-15},
  VOLUME = {59},
}

@ARTICLE{McNamara_Duffy-Deno_Marsh_Marsh_2019,
  ABSTRACT = {\&amp;lt;p\&amp;gt;We developed a dream content analysis system ({DCAS}) based on an artificial intelligence ({AI}) algorithm that was trained using a relatively large corpus of over 35,000 dreams.\&amp;amp;nbsp; This sample of dreams were supplied by 424 female and 211 male users over 4 years who had posted them at the dream posting website and app Dreamboard.com. Building upon previous dream content ontologies developed by Hall, Van de Castle, Domhoff and Bulkeley, forty-seven reliably identified dream themes emerged from repeated application of algorithm and agent training procedures. {DCAS} reproduced most of the key dream content themes from these previous ontologies but also returned some unexpected findings. Mixed-model estimation detected significant male-female content differences for 34 dream themes, with female dreams evidencing higher incidence percentages for most themes, but effect sizes were small. Mixed-model logistic regression identified those themes that best predicted self-reported positive or negative mood associated with dreams.\&amp;amp;nbsp; We conclude that the {AI}-based {DCAS} algorithm developed here is a promising tool for detailed analyses of dream content patterns.\&amp;lt;/p\&amp;gt;},
  AUTHOR = {{McNamara}, Patrick and Duffy-Deno, Kelly and Marsh, Tom and Marsh, Thomas Jr.},
  URL = {https://journals.ub.uni-heidelberg.de/index.php/IJoDR/article/view/48744},
  DATE = {2019-04},
  DOI = {10.11588/ijodr.2019.1.48744},
  JOURNALTITLE = {International Journal of Dream Research},
  NUMBER = {1},
  PAGES = {42--52},
  TITLE = {Dream content analysis using Artificial Intelligence},
  VOLUME = {12},
}

@ARTICLE{mentrey2022,
  AUTHOR = {Menétrey, Maëlan Q. and Mohammadi, Gelareh and Leitão, Joana and Vuilleumier, Patrik},
  URL = {https://doi.org/10.3389/fcomp.2022.773256},
  DATE = {2022-01},
  DOI = {10.3389/fcomp.2022.773256},
  JOURNALTITLE = {Frontiers in Computer Science},
  TITLE = {Emotion recognition in a multi-componential framework: The role of physiology},
  VOLUME = {4},
}

@ARTICLE{micheli2013,
  AUTHOR = {Micheli, Raphaël},
  URL = {https://doi.org/10.4000/semen.9795},
  DATE = {2013-04},
  DOI = {10.4000/semen.9795},
  JOURNALTITLE = {Semen},
  NUMBER = {35},
  TITLE = {Esquisse dune typologie des différents modes de sémiotisation verbale de lémotion},
}

@INPROCEEDINGS{miller-1994-wordnet,
  AUTHOR = {Miller, George A.},
  URL = {https://aclanthology.org/H94-1111},
  BOOKTITLE = {Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994},
  DATE = {1994},
  TITLE = {{WordNet}: A Lexical Database for English},
}

@ARTICLE{mitchell_debate_2023,
  ABSTRACT = {We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language—and the physical and social situations language encodes—in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
  AUTHOR = {Mitchell, Melanie and Krakauer, David C.},
  URL = {https://www.pnas.org/doi/full/10.1073/pnas.2215907120},
  DATE = {2023-03},
  DOI = {10.1073/pnas.2215907120},
  FILE = {Full Text PDF:/Users/bonard/switchdrive2/Zotero/storage/9YF6DL9E/Mitchell et Krakauer - 2023 - The debate over understanding in AI’s large langua.pdf:application/pdf},
  JOURNALTITLE = {Proceedings of the National Academy of Sciences},
  NOTE = {Publisher: Proceedings of the National Academy of Sciences},
  NUMBER = {13},
  PAGES = {e2215907120},
  TITLE = {The debate over understanding in {AI}’s large language models},
  URLDATE = {2023-12-08},
  VOLUME = {120},
}

@INPROCEEDINGS{salif,
  ABSTRACT = {Tweets pertaining to a single event, such as a national election, can number in the hundreds of millions. Automatically analyzing them is beneficial in many downstream natural language applications such as question answering and summarization. In this paper, we propose a new task: identifying purpose behind electoral tweets—why do people post election-oriented tweets? We show that identifying purpose is related to sentiment and emotion detection, but yet significantly different. Detecting purpose has a number of applications including detecting the mood of the electorate, estimating the popularity of policies, identifying key issues of contention, and predicting the course of events. We create a large dataset of electoral tweets and annotate a few thousand tweets for purpose. We develop a system that automatically classifies electoral tweets as per their purpose, obtaining an accuracy of 44.58},
  AUTHOR = {Mohammad, Saif M. and Kiritchenko, Svetlana and Martin, Joel},
  LOCATION = {Chicago, Illinois},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://doi.org/10.1145/2502069.2502070},
  BOOKTITLE = {Proceedings of the second international workshop on issues of sentiment discovery and opinion mining},
  DATE = {2013},
  DOI = {10.1145/2502069.2502070},
  ISBN = {978-1-4503-2332-1},
  NOTE = {Number of pages: 9 tex.address: New York, {NY}, {USA} tex.articleno: 1},
  SERIES = {Wisdom '13},
  TITLE = {Identifying purpose behind electoral tweets},
}

@BOOK{moors_demystifying_2022,
  AUTHOR = {Moors, Agnes},
  LOCATION = {Cambridge},
  DATE = {2022},
  EDITION = {Cambridge University Press},
  FILE = {Moors 2022 Demystifying Emotions.pdf:/Users/bonard/switchdrive2/bibliothèque/_Moors agnes articles livre + Duffy/Moors 2022 Demystifying emotions/Moors 2022 Demystifying Emotions.pdf:application/pdf},
  TITLE = {Demystifying emotions: {A} {Typology} of theories in psychology and philosophy},
}

@ARTICLE{moors_appraisal_2013,
  AUTHOR = {Moors, Agnes and Ellsworth, Phoebe C. and Scherer, Klaus R. and Frijda, Nico H.},
  DATE = {2013},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/SHA4VVDJ/Moors et al. - 2013 - Appraisal theories of emotion State of the art an.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/BHSHSZ6W/1754073912468165.html:text/html},
  JOURNALTITLE = {Emotion Review},
  NOTE = {Publisher: Sage Publications Sage UK: London, England},
  NUMBER = {2},
  PAGES = {119--124},
  SHORTTITLE = {Appraisal theories of emotion},
  TITLE = {Appraisal theories of emotion: state of the art and future development},
  VOLUME = {5},
}

@ARTICLE{mukherjee2023orca,
  AUTHOR = {Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
  DATE = {2023},
  EPRINT = {2306.02707 [cs.CL]},
  EPRINTTYPE = {arxiv},
  TITLE = {Orca: Progressive learning from complex explanation traces of {GPT}-4},
}

@ARTICLE{nealSurveyingStylometryTechniques2017,
  ABSTRACT = {The analysis of authorial style, termed stylometry, assumes that style is quantifiably measurable for evaluation of distinctive qualities. Stylometry research has yielded several methods and tools over the past 200 years to handle a variety of challenging cases. This survey reviews several articles within five prominent subtasks: authorship attribution, authorship verification, authorship profiling, stylochronometry, and adversarial stylometry. Discussions on datasets, features, experimental techniques, and recent approaches are provided. Further, a current research challenge lies in the inability of authorship analysis techniques to scale to a large number of authors with few text samples. Here, we perform an extensive performance analysis on a corpus of 1,000 authors to investigate authorship attribution, verification, and clustering using 14 algorithms from the literature. Finally, several remaining research challenges are discussed, along with descriptions of various open-source and commercial software that may be useful for stylometry subtasks.},
  AUTHOR = {Neal, Tempestt and Sundararajan, Kalaivani and Fatima, Aneez and Yan, Yiming and Xiang, Yingfei and Woodard, Damon},
  DATE = {2017},
  ISSN = {0360-0300},
  JOURNALTITLE = {Acm Computing Surveys},
  NUMBER = {6},
  PAGES = {86:1--86:36},
  SHORTJOURNAL = {ACM Comput. Surv.},
  TITLE = {Surveying Stylometry Techniques and Applications},
  VOLUME = {50},
}

@ARTICLE{oconnor_measurement_2019,
  AUTHOR = {O'Connor, Peter J. and Hill, Andrew and Kaya, Maria and Martin, Brett},
  URL = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.01116/full?amp=1},
  DATE = {2019},
  JOURNALTITLE = {Frontiers in psychology},
  NOTE = {Publisher: Frontiers},
  PAGES = {1116},
  SHORTTITLE = {The measurement of emotional intelligence},
  TITLE = {The measurement of emotional intelligence: {A} critical review of the literature and recommendations for researchers and practitioners},
  URLDATE = {2024-02-15},
  VOLUME = {10},
}

@INCOLLECTION{odonnell2016handbook,
  AUTHOR = {O’Donnell, Mick},
  EDITOR = {Thompson, Geoff and Bowcher, Wendy L. and Fontaine, Lise and Schönthal, David},
  LOCATION = {Cambridge},
  PUBLISHER = {Cambridge University Press},
  BOOKTITLE = {The Cambridge Handbook of Systemic Functional Linguistics},
  DATE = {2019},
  DOI = {10.1017/9781316585691.011},
  TITLE = {Continuing Issues in {{SFL}}},
}

@UNPUBLISHED{oberlanderExperiencersStimuliTargets2020,
  ABSTRACT = {Emotion recognition is predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory (e.g., fear, joy, anger, disgust, sadness, surprise, trust, anticipation). More recently, semantic role labeling approaches have been developed to extract structures from the text to answer questions like: "who is described to feel the emotion?" (experiencer), "what causes this emotion?" (stimulus), and at which entity is it directed?" (target). Though it has been shown that jointly modeling stimulus and emotion category prediction is beneficial for both subtasks, it remains unclear which of these semantic roles enables a classifier to infer the emotion. Is it the experiencer, because the identity of a person is biased towards a particular emotion (X is always happy)? Is it a particular target (everybody loves X) or a stimulus (doing X makes everybody sad)? We answer these questions by training emotion classification models on five available datasets annotated with at least one semantic role by masking the fillers of these roles in the text in a controlled manner and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification.},
  AUTHOR = {Oberländer, Laura and Reich, Kevin and Klinger, Roman},
  DATE = {2020-11},
  EPRINT = {2011.01599},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Computation and Language,notion},
  SHORTTITLE = {Experiencers, {{Stimuli}}, or {{Targets}}},
  TITLE = {Experiencers, {{Stimuli}}, or {{Targets}}: {{Which Semantic Roles Enable Machine Learning}} to {{Infer}} the {{Emotions}}?},
}

@ARTICLE{openai2023gpt4,
  AUTHOR = {{OpenAI}},
  DATE = {2023},
  JOURNALTITLE = {{PREPRINT}},
  TITLE = {{GPT}-4 technical report},
}

@MISC{paechEQBenchEmotionalIntelligence2024,
  ABSTRACT = {We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of emotional intelligence in Large Language Models (LLMs). We assess the ability of LLMs to understand complex emotions and social interactions by asking them to predict the intensity of emotional states of characters in a dialogue. The benchmark is able to discriminate effectively between a wide range of models. We find that EQ-Bench correlates strongly with comprehensive multi-domain benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may be capturing similar aspects of broad intelligence. Our benchmark produces highly repeatable results using a set of 60 English-language questions. We also provide open-source code for an automated benchmarking pipeline at https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com},
  AUTHOR = {Paech, Samuel J.},
  ORGANIZATION = {arXiv},
  DATE = {2024-01},
  DOI = {10.48550/arXiv.2312.06281},
  EPRINT = {2312.06281},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,I.2.7},
  SHORTTITLE = {{{EQ-Bench}}},
  TITLE = {{{EQ-Bench}}: {{An Emotional Intelligence Benchmark}} for {{Large Language Models}}},
}

@BOOK{panksepp_affective_1998,
  AUTHOR = {Panksepp, Jaak},
  LOCATION = {New York},
  PUBLISHER = {Oxford University Press},
  DATE = {1998},
  SHORTTITLE = {Affective neuroscience},
  TITLE = {Affective neuroscience: the foundations of human and animal emotions},
}

@MISC{parkGenerativeAgentsInteractive2023a,
  ABSTRACT = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents–computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture–observation, planning, and reflection–each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  AUTHOR = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  ORGANIZATION = {arXiv},
  DATE = {2023-08},
  DOI = {10.48550/arXiv.2304.03442},
  EPRINT = {2304.03442},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  SHORTTITLE = {Generative {{Agents}}},
  TITLE = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
}

@INPROCEEDINGS{paroubek-etal-2018-deft2018,
  AUTHOR = {Paroubek, Patrick and Grouin, Cyril and Bellot, Patrice and Claveau, Vincent and Eshkol-Taravella, Iris and Fraisse, Amel and Jackiewicz, Agata and Karoui, Jihen and Monceaux, Laura and Torres-Moreno, Juan-Manuel},
  LANGUAGE = {French},
  LOCATION = {Rennes, France},
  PUBLISHER = {ATALA},
  URL = {https://aclanthology.org/2018.jeptalnrecital-deft.1},
  BOOKTITLE = {Actes de la Conférence TALN. Volume 2 - Démonstrations, articles des Rencontres Jeunes Chercheurs, ateliers DeFT},
  DATE = {2018-05},
  PAGES = {219--230},
  TITLE = {{DEFT}2018 : recherche d{'}information et analyse de sentiments dans des tweets concernant les transports en {Î}le de {F}rance ({DEFT}2018 : Information Retrieval and Sentiment Analysis in Tweets about Public Transportation in {Î}le de {F}rance Region )},
}

@MISC{paszkePyTorchImperativeStyle2019,
  ABSTRACT = {Deep learning frameworks have often focused on either usability or speed, but not both. {PyTorch} is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as {GPUs}. In this paper, we detail the principles that drove the implementation of {PyTorch} and how they are reflected in its architecture. We emphasize that every aspect of {PyTorch} is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of {PyTorch} on several common benchmarks.},
  AUTHOR = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and {DeVito}, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/1912.01703},
  DATE = {2019-12-03},
  DOI = {10.48550/arXiv.1912.01703},
  EPRINT = {1912.01703 [cs]},
  EPRINTTYPE = {arxiv},
  FILE = {Preprint PDF:C\:\\Users\\Gustave\\Zotero\\storage\\P3S3AAXS\\Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Deep Learning Library.pdf:application/pdf;Snapshot:C\:\\Users\\Gustave\\Zotero\\storage\\F4LD4X8I\\1912.html:text/html},
  KEYWORDS = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Machine Learning},
  NUMBER = {{arXiv}:1912.01703},
  SHORTTITLE = {{PyTorch}},
  TITLE = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
  URLDATE = {2025-09-24},
}

@ARTICLE{scikit-learn,
  AUTHOR = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  DATE = {2011},
  JOURNALTITLE = {Journal of Machine Learning Research},
  PAGES = {2825--2830},
  TITLE = {Scikit-learn: Machine learning in Python},
  VOLUME = {12},
}

@BOOK{pinHandbookAutomataTheory2021,
  EDITOR = {Pin, Jean-Eric},
  PUBLISHER = {EMS Press},
  DATE = {2021},
  ISBN = {978-3-98547-003-7 978-3-98547-503-2},
  TITLE = {Handbook of Automata Theory},
  VOLUME = {2},
}

@ARTICLE{plutchikNatureEmotionsHuman2001,
  AUTHOR = {Plutchik, Robert},
  PUBLISHER = {Sigma Xi, The Scientific Research Society},
  DATE = {2001},
  EPRINT = {27857503},
  EPRINTTYPE = {JSTOR},
  ISSN = {0003-0996},
  JOURNALTITLE = {American Scientist},
  NUMBER = {4},
  PAGES = {344--350},
  SHORTTITLE = {The {{Nature}} of {{Emotions}}},
  TITLE = {The {{Nature}} of {{Emotions}}: {{Human}} Emotions Have Deep Evolutionary Roots, a Fact That May Explain Their Complexity and Provide Tools for Clinical Practice},
  VOLUME = {89},
}

@ARTICLE{plutchik,
  AUTHOR = {Plutchik, Robert},
  URL = {http://www.jstor.org/stable/27857503},
  DATE = {2001},
  ISSN = {00030996},
  JOURNALTITLE = {American Scientist},
  NOTE = {Publisher: Sigma Xi, The Scientific Research Society},
  NUMBER = {4},
  PAGES = {344--350},
  TITLE = {The Nature of Emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice},
  URLDATE = {2022-06-13},
  VOLUME = {89},
}

@MISC{poriaTipIcebergCurrent2020a,
  ABSTRACT = {Sentiment analysis as a field has come a long way since it was first introduced as a task nearly 20 years ago. It has widespread commercial applications in various domains like marketing, risk management, market research, and politics, to name a few. Given its saturation in specific subtasks – such as sentiment polarity classification – and datasets, there is an underlying perception that this field has reached its maturity. In this article, we discuss this perception by pointing out the shortcomings and under-explored, yet key aspects of this field that are necessary to attain true sentiment understanding. We analyze the significant leaps responsible for its current relevance. Further, we attempt to chart a possible course for this field that covers many overlooked and unanswered questions.},
  AUTHOR = {Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Mihalcea, Rada},
  ORGANIZATION = {arXiv},
  DATE = {2020-11},
  DOI = {10.48550/arXiv.2005.00357},
  EPRINT = {2005.00357},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  SHORTTITLE = {Beneath the {{Tip}} of the {{Iceberg}}},
  TITLE = {Beneath the {{Tip}} of the {{Iceberg}}: {{Current Challenges}} and {{New Directions}} in {{Sentiment Analysis Research}}},
}

@ARTICLE{beneath,
  AUTHOR = {Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Mihalcea, Rada},
  DATE = {2023},
  DOI = {10.1109/TAFFC.2020.3038167},
  JOURNALTITLE = {{IEEE} Transactions on Affective Computing},
  NUMBER = {1},
  PAGES = {108--132},
  TITLE = {Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research},
  VOLUME = {14},
}

@ARTICLE{2020t5,
  AUTHOR = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  URL = {http://jmlr.org/papers/v21/20-074.html},
  DATE = {2020},
  JOURNALTITLE = {Journal of Machine Learning Research},
  NUMBER = {140},
  PAGES = {1--67},
  TITLE = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  VOLUME = {21},
}

@INPROCEEDINGS{reimers-2019-sentence-bert,
  AUTHOR = {Reimers, Nils and Gurevych, Iryna},
  PUBLISHER = ACL,
  URL = {http://arxiv.org/abs/1908.10084},
  BOOKTITLE = {Proceedings of the 2019 conference on empirical methods in natural language processing},
  DATE = {2019-11},
  TITLE = {Sentence-{BERT}: Sentence embeddings using siamese {BERT}-networks},
}

@ARTICLE{Reinert93,
  AUTHOR = {Reinert, Max},
  URL = {https://www.persee.fr/doc/lsoc_0181-4095_1993_num_66_1_2632},
  DATE = {1993},
  JOURNALTITLE = {Langage \& société},
  PAGES = {5--39},
  TITLE = {Les "mondes lexicaux" et leur "logique" à travers l'analyse statistique d'un corpus de récits de cauchemars},
  VOLUME = {66},
}

@ARTICLE{russel,
  AUTHOR = {Russell, James A and Mehrabian, Albert},
  URL = {https://www.sciencedirect.com/science/article/pii/009265667790037X},
  DATE = {1977},
  DOI = {https://doi.org/10.1016/0092-6566(77)90037-X},
  ISSN = {0092-6566},
  JOURNALTITLE = {Journal of Research in Personality},
  NUMBER = {3},
  PAGES = {273--294},
  TITLE = {Evidence for a three-factor theory of emotions},
  VOLUME = {11},
}

@ARTICLE{russell_core_1999,
  ABSTRACT = {What is the structure of emotion? Emotion is too broad a class of events to be a single scientific category, and no one structure suffices. As an illustration, core affect is distinguished from prototypical emotional episode. Core affect refers to consciously accessible elemental processes of pleasure and activation, has many causes, and is always present. Its structure involves two bipolar dimensions. Prototypical emotional episode refers to a complex process that unfolds over time, involves causally connected subevents (antecedent; appraisal; physiological, affective, and cognitive changes; behavioral response; self-categorization), has one perceived cause, and is rare. Its structure involves categories (anger, fear, shame, jealousy, etc.) vertically organized as a fuzzy hierarchy and horizontally organized as part of a circumplex.},
  AUTHOR = {Russell, James A. and Barrett, Lisa},
  DATE = {1999-06},
  DOI = {10.1037//0022-3514.76.5.805},
  FILE = {Full Text PDF:/Users/bonard/switchdrive2/Zotero/storage/I6M5JR2P/Russell et Barrett - 1999 - Core affect, prototypical emotional episodes, and .pdf:application/pdf},
  JOURNALTITLE = {Journal of personality and social psychology},
  PAGES = {805--19},
  SHORTTITLE = {Core affect, prototypical emotional episodes, and other things called emotion},
  TITLE = {Core affect, prototypical emotional episodes, and other things called emotion: {Dissecting} the elephant},
  VOLUME = {76},
}

@ARTICLE{10.3389/fnins.2018.00007,
  ABSTRACT = {Ever since the modern rediscovery of psychedelic substances by Western society, several authors have independently proposed that their effects bear a high resemblance to the dreams and dreamlike experiences occurring naturally during the sleep-wake cycle. Recent studies in humans have provided neurophysiological evidence supporting this hypothesis. However, a rigorous comparative analysis of the phenomenology (“what it feels like” to experience these states) is currently lacking. We investigated the semantic similarity between a large number of subjective reports of psychoactive substances and reports of high/low lucidity dreams, and found that the highest-ranking substance in terms of the similarity to high lucidity dreams was the serotonergic psychedelic lysergic acid diethylamide ({LSD}), whereas the highest-ranking in terms of the similarity to dreams of low lucidity were plants of the Datura genus, rich in deliriant tropane alkaloids. Conversely, sedatives, stimulants, antipsychotics, and antidepressants comprised most of the lowest-ranking substances. An analysis of the most frequent words in the subjective reports of dreams and hallucinogens revealed that terms associated with perception (“see,” “visual,” “face,” “reality,” “color”), emotion (“fear”), setting (“outside,” “inside,” “street,” “front,” “behind”) and relatives (“mom,” “dad,” “brother,” “parent,” “family”) were the most prevalent across both experiences. In summary, we applied novel quantitative analyses to a large volume of empirical data to confirm the hypothesis that, among all psychoactive substances, hallucinogen drugs elicit experiences with the highest semantic similarity to those of dreams. Our results and the associated methodological developments open the way to study the comparative phenomenology of different altered states of consciousness and its relationship with non-invasive measurements of brain physiology.},
  AUTHOR = {Sanz, Camila and Zamberlan, Federico and Erowid, Earth and Erowid, Fire and Tagliazucchi, Enzo},
  URL = {https://www.frontiersin.org/articles/10.3389/fnins.2018.00007},
  DATE = {2018},
  DOI = {10.3389/fnins.2018.00007},
  ISSN = {1662-453X},
  JOURNALTITLE = {Frontiers in Neuroscience},
  TITLE = {The experience elicited by hallucinogens presents the highest similarity to dreaming within a large database of psychoactive substance reports},
  VOLUME = {12},
}

@BOOK{Saussure1916,
  AUTHOR = {de Saussure, Ferdinand},
  LOCATION = {Paris},
  PUBLISHER = {Payot},
  DATE = {1916},
  KEYWORDS = {linguistics},
  TITLE = {Cours de Linguistique Générale},
}

@ARTICLE{scarantino_how_2017,
  AUTHOR = {Scarantino, Andrea},
  DATE = {2017},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/IXV5R75Y/Scarantino 2017 How to Do Things with Emotional Expressions –The Theory of Affective Pragmatics.pdf:text/html;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/EAE7GS89/1047840X.2017.html:text/html},
  JOURNALTITLE = {Psychological Inquiry},
  NOTE = {Publisher: Taylor \& Francis},
  NUMBER = {2-3},
  PAGES = {165--185},
  SHORTTITLE = {How to do things with emotional expressions},
  TITLE = {How to do things with emotional expressions: {The} theory of affective pragmatics},
  VOLUME = {28},
}

@ARTICLE{schachter_cognitive_1962,
  AUTHOR = {Schachter, Stanley and Singer, Jerome},
  DATE = {1962},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/VZF77N6F/Schachter et Singer - 1962 - Cognitive, social, and physiological determinants .pdf:application/pdf},
  JOURNALTITLE = {Psychological review},
  NOTE = {Publisher: American Psychological Association},
  NUMBER = {5},
  PAGES = {379},
  TITLE = {Cognitive, social, and physiological determinants of emotional state.},
  VOLUME = {69},
}

@ARTICLE{component,
  AUTHOR = {Scherer, Klaus R.},
  URL = {https://doi.org/10.1177/0539018405058216},
  DATE = {2005},
  DOI = {10.1177/0539018405058216},
  JOURNALTITLE = {Social Science Information},
  NOTE = {tex.eprint: https://doi.org/10.1177/0539018405058216},
  NUMBER = {4},
  PAGES = {695--729},
  TITLE = {What are emotions? And how can they be measured?},
  VOLUME = {44},
}

@ARTICLE{scherer_componential_2007,
  AUTHOR = {Scherer, Klaus R.},
  URL = {https://psycnet.apa.org/record/2007-12449-004},
  DATE = {2007},
  FILE = {Scherer 2007 Componencial emotion theory can inform models of emotional competence.pdf:/Users/bonard/switchdrive2/academia/bibliothèque académique/Scherer 2007 Componencial emotion theory can inform models of emotional competence.pdf:application/pdf},
  NOTE = {Publisher: Oxford University Press},
  TITLE = {Componential emotion theory can inform models of emotional competence.},
  URLDATE = {2024-02-15},
}

@ARTICLE{schererTheoryConvergenceEmotion2022,
  ABSTRACT = {Over the last century, emotion research has been beset by the problem of major disagreements with respect to the definition of the phenomenon and an abundance of different theories. Arguably, these divergences have had adverse effects on theory development, on the theoretical foundations of empirical research, and on knowledge accumulation in the study of emotion. Similar problems have been encountered in other areas of behavioural science. Increasingly, there have been calls to work towards some form of theory integration. In contrast, here an effort is made to show that a reasonable degree of theory convergence in the area of emotion science can be attained by adopting a design feature-based working definition of emotion and highlighting the basic agreement on the components of the dynamic emotion process. The aim is to invite constructive discussion on communalities and divergences between different theories and foster the development of more complementary theoretical frameworks to guide future empirical research.},
  AUTHOR = {Scherer, Klaus R.},
  PUBLISHER = {Routledge},
  DATE = {2022-02},
  DOI = {10.1080/02699931.2021.1973378},
  EPRINT = {35188091},
  EPRINTTYPE = {pubmed},
  ISSN = {0269-9931},
  JOURNALTITLE = {Cognition and Emotion},
  KEYWORDS = {emotion components,emotion definition,emotion process,Emotion theories,theory convergence},
  NUMBER = {2},
  PAGES = {154--170},
  TITLE = {Theory Convergence in Emotion Science Is Timely and Realistic},
  VOLUME = {36},
}

@ARTICLE{scherer_emotion_2019,
  AUTHOR = {Scherer, Klaus R. and Moors, Agnes},
  DATE = {2019},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/R9B4IVYY/Scherer et Moors - 2019 - The emotion process Event appraisal and component.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/MZ7VT9PS/annurev-psych-122216-011854.html:text/html},
  JOURNALTITLE = {Annual Review of Psychology},
  NOTE = {Publisher: Annual Reviews},
  PAGES = {719--745},
  SHORTTITLE = {The emotion process},
  TITLE = {The emotion process: event appraisal and component differentiation},
  VOLUME = {70},
}

@ARTICLE{Scherer1994,
  AUTHOR = {Scherer, Klaus R. and Wallbott, Harald G.},
  URL = {https://doi.org/10.1037/0022-3514.66.2.310},
  DATE = {1994},
  DOI = {10.1037/0022-3514.66.2.310},
  JOURNALTITLE = {Journal of Personality and Social Psychology},
  NOTE = {Publisher: American Psychological Association ({APA})},
  NUMBER = {2},
  PAGES = {310--328},
  TITLE = {Evidence for universality and cultural variation of differential emotion response patterning.},
  VOLUME = {66},
}

@MISC{schickToolformerLanguageModels2023,
  ABSTRACT = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\textbackslash\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  AUTHOR = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  ORGANIZATION = {arXiv},
  DATE = {2023-02},
  DOI = {10.48550/arXiv.2302.04761},
  EPRINT = {2302.04761},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Computation and Language},
  SHORTTITLE = {Toolformer},
  TITLE = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
}

@INCOLLECTION{aloni_semantics-pragmatics_2016,
  AUTHOR = {Schlenker, Philippe},
  EDITOR = {Aloni, Maria and Dekker, Paul},
  LOCATION = {Cambridge},
  PUBLISHER = {Cambridge University Press},
  BOOKTITLE = {The {Cambridge} {Handbook} of {Formal} {Semantics}},
  DATE = {2016},
  FILE = {schlenker_16_The Semantics-pragmatics interface.pdf:/Users/bonard/switchdrive2/academia/bibliothèque académique/schlenker_16_The Semantics-pragmatics interface.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/BGL7LVP5/books.html:text/html},
  PAGES = {664--727},
  TITLE = {The semantics-pragmatics interface},
}

@ARTICLE{continuity,
  AUTHOR = {Schredl, Michael and Hofmann, Friedrich},
  URL = {https://doi.org/10.1016/s1053-8100(02)00072-7},
  DATE = {2003-06},
  DOI = {10.1016/s1053-8100(02)00072-7},
  JOURNALTITLE = {Consciousness and Cognition},
  NOTE = {Publisher: Elsevier {BV}},
  NUMBER = {2},
  PAGES = {298--308},
  TITLE = {Continuity between waking activities and dream activities},
  VOLUME = {12},
}

@ARTICLE{shanahanRolePlayLarge2023,
  ABSTRACT = {As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness.},
  AUTHOR = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
  PUBLISHER = {Nature Publishing Group},
  DATE = {2023-11},
  DOI = {10.1038/s41586-023-06647-8},
  ISSN = {1476-4687},
  JOURNALTITLE = {Nature},
  KEYWORDS = {Computer science,Philosophy},
  LANGID = {english},
  NUMBER = {7987},
  PAGES = {493--498},
  TITLE = {Role Play with Large Language Models},
  VOLUME = {623},
}

@MISC{shapira_clever_2023,
  ABSTRACT = {The escalating debate on AI's capabilities warrants developing reliable metrics to assess machine "intelligence". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.},
  AUTHOR = {Shapira, Natalie and Levy, Mosh and Alavi, Seyed Hossein and Zhou, Xuhui and Choi, Yejin and Goldberg, Yoav and Sap, Maarten and Shwartz, Vered},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/2305.14763},
  DATE = {2023-05},
  DOI = {10.48550/arXiv.2305.14763},
  FILE = {arXiv Fulltext PDF:/Users/bonard/switchdrive2/Zotero/storage/4UCIJR9E/Shapira et al. - 2023 - Clever Hans or Neural Theory of Mind Stress Testi.pdf:application/pdf},
  KEYWORDS = {Computer Science - Computation and Language},
  NOTE = {arXiv:2305.14763 [cs]},
  SHORTTITLE = {Clever {Hans} or {Neural} {Theory} of {Mind}?},
  TITLE = {Clever {Hans} or {Neural} {Theory} of {Mind}? {Stress} {Testing} {Social} {Reasoning} in {Large} {Language} {Models}},
  URLDATE = {2023-12-08},
}

@MISC{sharmaCognitiveReframingNegative2023a,
  ABSTRACT = {A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful "reframed thought." Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people's access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a "high-quality" reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.},
  AUTHOR = {Sharma, Ashish and Rushton, Kevin and Lin, Inna Wanyin and Wadden, David and Lucas, Khendra G. and Miner, Adam S. and Nguyen, Theresa and Althoff, Tim},
  ORGANIZATION = {arXiv},
  DATE = {2023-05},
  DOI = {10.48550/arXiv.2305.02466},
  EPRINT = {2305.02466},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Social and Information Networks},
  TITLE = {Cognitive {{Reframing}} of {{Negative Thoughts}} through {{Human-Language Model Interaction}}},
}

@INPROCEEDINGS{sharmaCognitiveReframingNegative2023b,
  ABSTRACT = {A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful “reframed thought.” Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people's access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a “high-quality” reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts.},
  AUTHOR = {Sharma, Ashish and Rushton, Kevin and Lin, Inna and Wadden, David and Lucas, Khendra and Miner, Adam and Nguyen, Theresa and Althoff, Tim},
  EDITOR = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  LOCATION = {Toronto, Canada},
  PUBLISHER = ACL,
  BOOKTITLE = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  DATE = {2023},
  FILE = {C:\Users\Gustave\Zotero\storage\KUDKPFE5\Sharma et al. - 2023 - Cognitive Reframing of Negative Thoughts through H.pdf},
  PAGES = {9977--10000},
  TITLE = {Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction},
}

@INPROCEEDINGS{DBLP:conf/wassa/SinghHWM21,
  AUTHOR = {Singh, Aaditya and Hingane, Shreeshail and Wani, Saim and Modi, Ashutosh},
  EDITOR = {Clercq, Orphée De and Balahur, Alexandra and Sedoc, João and Barrière, Valentin and Tafreshi, Shabnam and Buechel, Sven and Hoste, Véronique},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2021.wassa-1.9/},
  BOOKTITLE = {Proceedings of the eleventh workshop on computational approaches to subjectivity, sentiment and social media analysis, {WASSA}@{EACL} 2021, online, april 19, 2021},
  DATE = {2021},
  NOTE = {tex.bibsource: dblp computer science bibliography, https://dblp.org tex.timestamp: Fri, 06 Aug 2021 00:40:11 +0200},
  PAGES = {84--91},
  TITLE = {An end-to-end network for emotion-cause pair extraction},
}

@BOOK{sperber_relevance_1995,
  AUTHOR = {Sperber, Dan and Wilson, Deirdre},
  LOCATION = {Oxford and Cambridge (MA)},
  PUBLISHER = {Blackwell},
  DATE = {1995},
  EDITION = {2nd Edition},
  FILE = {Sperber Wilson 1995 Relevance.pdf:/Users/bonard/switchdrive2/bibliothèque/Sperber Wilson 1995 Relevance – communication and cognition.pdf:application/pdf},
  SHORTTITLE = {Relevance},
  TITLE = {Relevance: {Communication} and cognition},
}

@ARTICLE{stalnaker_common_2002,
  AUTHOR = {Stalnaker, Robert},
  DATE = {2002},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/X8MYGHZB/Stalnaker - 2002 - Common ground.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/KTRYQGXH/25001871.html:text/html},
  JOURNALTITLE = {Linguistics and philosophy},
  NUMBER = {5/6},
  PAGES = {701--721},
  TITLE = {Common ground},
  VOLUME = {25},
}

@ARTICLE{stojnic_commonsense_2023,
  ABSTRACT = {Human infants are fascinated by other people. They bring to this fascination a constellation of rich and flexible expectations about the intentions motivating people's actions. Here we test 11-month-old infants and state-of-the-art learning-driven neural-network models on the “Baby Intuitions Benchmark (BIB),” a suite of tasks challenging both infants and machines to make high-level predictions about the underlying causes of agents' actions. Infants expected agents' actions to be directed towards objects, not locations, and infants demonstrated default expectations about agents' rationally efficient actions towards goals. The neural-network models failed to capture infants' knowledge. Our work provides a comprehensive framework in which to characterize infants' commonsense psychology and takes the first step in testing whether human knowledge and human-like artificial intelligence can be built from the foundations cognitive and developmental theories postulate.},
  AUTHOR = {Stojnić, Gala and Gandhi, Kanishk and Yasuda, Shannon and Lake, Brenden M. and Dillon, Moira R.},
  URL = {https://www.sciencedirect.com/science/article/pii/S0010027723000409},
  DATE = {2023},
  DOI = {https://doi.org/10.1016/j.cognition.2023.105406},
  FILE = {Stojnić et al 2023 Commonsense psychology in human infants and machines.pdf:/Users/bonard/switchdrive2/academia/bibliothèque académique/Stojnić et al 2023 Commonsense psychology in human infants and machines.pdf:application/pdf},
  ISSN = {0010-0277},
  JOURNALTITLE = {Cognition},
  KEYWORDS = {Infancy,Artificial intelligence,Action understanding,Commonsense psychology,Intuitive psychology,Machine common sense},
  PAGES = {105406},
  TITLE = {Commonsense psychology in human infants and machines},
  VOLUME = {235},
}

@ARTICLE{teigen_is_2008,
  AUTHOR = {Teigen, Karl Halvor},
  DATE = {2008},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/C325HXX5/Teigen - 2008 - Is a sigh “just a sigh” Sighs as emotional signal.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/9FJ2ULGW/j.1467-9450.2007.00599.html:text/html},
  JOURNALTITLE = {Scandinavian journal of Psychology},
  NOTE = {Publisher: Wiley Online Library},
  NUMBER = {1},
  PAGES = {49--57},
  SHORTTITLE = {Is a sigh “just a sigh”?},
  TITLE = {Is a sigh “just a sigh”? {Sighs} as emotional signals and responses to a difficult task},
  VOLUME = {49},
}

@INCOLLECTION{TellierFinkel95,
  AUTHOR = {Tellier, Isabelle and Finkel, Alain},
  EDITOR = {Endres-Niggemeyer, Heide},
  BOOKTITLE = {The Cognitive Level},
  DATE = {1995},
  PAGES = {41--59},
  SERIES = {Duisburg {{LAUD}} Series of Cognitive Linguistics},
  TITLE = {Cognitive Style of Decision Making Narrations},
}

@BOOK{tesniereElementsStructuralSyntax2015,
  AUTHOR = {Tesnière, Lucien},
  TRANSLATOR = {Osborne, Timothy and Kahane, Sylvain},
  LOCATION = {Amsterdam},
  PUBLISHER = {John Benjamins Publishing Company},
  DATE = {2015},
  FILE = {C:\Users\Gustave\Zotero\storage\4XQ96JW9\Tesnière - 2015 - Elements of Structural Syntax.pdf},
  ISBN = {978-90-272-1212-2 978-90-272-6999-7},
  LANGID = {english},
  TITLE = {Elements of Structural Syntax},
}

@INPROCEEDINGS{simulation1,
  AUTHOR = {Thill, Serge and Svensson, Henrik},
  BOOKTITLE = {Proceedings of the annual meeting of the cognitive science society},
  DATE = {2011},
  TITLE = {The inception of simulation: a hypothesis for the role of dreams in young children},
  VOLUME = {33},
}

@BOOK{tomkins_affect_1962,
  AUTHOR = {Tomkins, Silvan},
  LOCATION = {New York},
  PUBLISHER = {Springer},
  DATE = {1962},
  SHORTTITLE = {Affect imagery consciousness},
  TITLE = {Affect imagery consciousness},
  URLDATE = {2024-02-12},
  VOLUME = {Volume I: The positive affects},
}

@ARTICLE{touvron2023llama,
  AUTHOR = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  DATE = {2023},
  EPRINT = {2307.09288 [cs.CL]},
  EPRINTTYPE = {arxiv},
  TITLE = {Llama 2: Open foundation and fine-tuned chat models},
}

@ARTICLE{troianoDimensionalModelingEmotions2022,
  ABSTRACT = {The most prominent tasks in emotion analysis are to assign emotions to texts and to understand how emotions manifest in language. An observation for NLP is that emotions can be communicated implicitly by referring to events, appealing to an empathetic, intersubjective understanding of events, even without explicitly mentioning an emotion name. In psychology, the class of emotion theories known as appraisal theories aims at explaining the link between events and emotions. Appraisals can be formalized as variables that measure a cognitive evaluation by people living through an event that they consider relevant. They include the assessment if an event is novel, if the person considers themselves to be responsible, if it is in line with the own goals, and many others. Such appraisals explain which emotions are developed based on an event, e.g., that a novel situation can induce surprise or one with uncertain consequences could evoke fear. We analyze the suitability of appraisal theories for emotion analysis in text with the goal of understanding if appraisal concepts can reliably be reconstructed by annotators, if they can be predicted by text classifiers, and if appraisal concepts help to identify emotion categories. To achieve that, we compile a corpus by asking people to textually describe events that triggered particular emotions and to disclose their appraisals. Then, we ask readers to reconstruct emotions and appraisals from the text. This setup allows us to measure if emotions and appraisals can be recovered purely from text and provides a human baseline. Our comparison of text classification methods to human annotators shows that both can reliably detect emotions and appraisals with similar performance. Therefore, appraisals constitute an alternative computational emotion analysis paradigm and further improve the categorization of emotions in text with joint models.},
  AUTHOR = {Troiano, Enrica and Oberländer, Laura and Klinger, Roman},
  DATE = {2022-09},
  DOI = {10.1162/coli_a_00461},
  EPRINT = {2206.05238},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  ISSN = {0891-2017, 1530-9312},
  JOURNALTITLE = {Computational Linguistics},
  KEYWORDS = {Computer Science - Computation and Language,notion},
  PAGES = {1--71},
  SHORTTITLE = {Dimensional {{Modeling}} of {{Emotions}} in {{Text}} with {{Appraisal Theories}}},
  TITLE = {Dimensional {{Modeling}} of {{Emotions}} in {{Text}} with {{Appraisal Theories}}: {{Corpus Creation}}, {{Annotation Reliability}}, and {{Prediction}}},
}

@ARTICLE{troiano-etal-2023-dimensional,
  AUTHOR = {Troiano, Enrica and Oberländer, Laura and Klinger, Roman},
  LOCATION = {Cambridge, MA},
  PUBLISHER = {MIT Press},
  URL = {https://aclanthology.org/2023.cl-1.1},
  DATE = {2023-03},
  DOI = {10.1162/coli_a_00461},
  JOURNALTITLE = {Computational Linguistics},
  NUMBER = {1},
  PAGES = {1--72},
  TITLE = {Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction},
  VOLUME = {49},
}

@ARTICLE{10.1162/coli_a_00461,
  AUTHOR = {Troiano, Enrica and Oberländer*, Laura and Klinger, Roman},
  URL = {https://doi.org/10.1162/coli_a_00461},
  DATE = {2022-12},
  DOI = {10.1162/coli_a_00461},
  ISSN = {0891-2017},
  JOURNALTITLE = {Computational Linguistics},
  NOTE = {tex.eprint: https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli{\textbackslash}\_a{\textbackslash}\_00461/2062690/coli{\textbackslash}\_a{\textbackslash}\_00461.pdf},
  PAGES = {1--72},
  TITLE = {Dimensional modeling of emotions in text with appraisal theories: Corpus creation, annotation reliability, and prediction},
}

@ARTICLE{troianoTheoriesStylesTheir2023a,
  ABSTRACT = {Humans are naturally endowed with the ability to write in a particular style. They can, for instance, rephrase a formal letter in an informal way, convey a literal message with the use of figures of speech or edit a novel by mimicking the style of some well-known authors. Automating this form of creativity constitutes the goal of style transfer. As a natural language generation task, style transfer aims at rewriting existing texts, and specifically, it creates paraphrases that exhibit some desired stylistic attributes. From a practical perspective, it envisions beneficial applications, like chatbots that modulate their communicative style to appear empathetic, or systems that automatically simplify technical articles for a non-expert audience.Several style-aware paraphrasing methods have attempted to tackle style transfer. A handful of surveys give a methodological overview of the field, but they do not support researchers to focus on specific styles. With this paper, we aim at providing a comprehensive discussion of the styles that have received attention in the transfer task. We organize them in a hierarchy, highlighting the challenges for the definition of each of them and pointing out gaps in the current research landscape. The hierarchy comprises two main groups. One encompasses styles that people modulate arbitrarily, along the lines of registers and genres. The other group corresponds to unintentionally expressed styles, due to an author’s personal characteristics. Hence, our review shows how these groups relate to one another and where specific styles, including some that have not yet been explored, belong in the hierarchy. Moreover, we summarize the methods employed for different stylistic families, hinting researchers towards those that would be the most fitting for future research.},
  AUTHOR = {Troiano, Enrica and Velutharambath, Aswathy and Klinger, Roman},
  DATE = {2023},
  FILE = {C:\Users\Gustave\Zotero\storage\ZNSMNLBY\Troiano et al. - 2023 - From theories on styles to their transfer in text.pdf},
  ISSN = {1351-3249, 1469-8110},
  JOURNALTITLE = {Natural Language Engineering},
  KEYWORDS = {Natural Language Generation,Paraphrasing,Style Transfer,Translation Technology},
  LANGID = {english},
  NUMBER = {4},
  PAGES = {849--908},
  SHORTTITLE = {From Theories on Styles to Their Transfer in Text},
  TITLE = {From Theories on Styles to Their Transfer in Text: {{Bridging}} the Gap with a Hierarchical Survey},
  VOLUME = {29},
}

@ARTICLE{trott_large_2022,
  AUTHOR = {Trott, Sean and Jones, Cameron and Chang, Tyler and Michaelov, James and Bergen, Benjamin},
  DATE = {2022},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/J5VC64ZR/Trott et al. - 2022 - Do Large Language Models know what humans know.pdf:application/pdf},
  JOURNALTITLE = {arXiv preprint arXiv:2209.01515},
  TITLE = {Do {Large} {Language} {Models} know what humans know?},
}

@MISC{ullman_large_2023,
  ABSTRACT = {Intuitive psychology is a pillar of common-sense reasoning. The replication of this reasoning in machine intelligence is an important stepping-stone on the way to human-like artificial intelligence. Several recent tasks and benchmarks for examining this reasoning in Large-Large Models have focused in particular on belief attribution in Theory-of-Mind tasks. These tasks have shown both successes and failures. We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head. We argue that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates. We also consider what possible future successes on Theory-of-Mind tasks by more powerful LLMs would mean for ToM tasks with people.},
  AUTHOR = {Ullman, Tomer},
  PUBLISHER = {arXiv},
  URL = {http://arxiv.org/abs/2302.08399},
  DATE = {2023-03},
  FILE = {arXiv Fulltext PDF:/Users/bonard/switchdrive2/Zotero/storage/U8IVFTKS/Ullman - 2023 - Large Language Models Fail on Trivial Alterations .pdf:application/pdf},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,I.2.7},
  NOTE = {arXiv:2302.08399 [cs]},
  TITLE = {Large {Language} {Models} {Fail} on {Trivial} {Alterations} to {Theory}-of-{Mind} {Tasks}},
  URLDATE = {2023-12-08},
}

@MISC{vaswani2017attention,
  AUTHOR = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  DATE = {2017},
  EPRINT = {1706.03762 [cs.CL]},
  EPRINTTYPE = {arxiv},
  TITLE = {Attention is all you need},
}

@INPROCEEDINGS{NIPS2017_3f5ee243,
  AUTHOR = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  EDITOR = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  BOOKTITLE = {Advances in neural information processing systems},
  DATE = {2017},
  TITLE = {Attention is all you need},
  VOLUME = {30},
}

@ARTICLE{vlemincx_why_2009,
  AUTHOR = {Vlemincx, Elke and Van Diest, Ilse and De Peuter, Steven and Bresseleers, Johan and Bogaerts, Katleen and Fannes, Stien and Li, Wan and Van Den Bergh, Omer},
  DATE = {2009},
  FILE = {Snapshot:/Users/bonard/switchdrive2/Zotero/storage/QQ2SE5DH/j.1469-8986.2009.00842.html:text/html},
  JOURNALTITLE = {Psychophysiology},
  NOTE = {Publisher: Wiley Online Library},
  NUMBER = {5},
  PAGES = {1005--1013},
  SHORTTITLE = {Why do you sigh?},
  TITLE = {Why do you sigh? {Sigh} rate during induced stress and relief},
  VOLUME = {46},
}

@BOOK{walesDictionaryStylistics2014a,
  AUTHOR = {Wales, Katie},
  PUBLISHER = {Routledge},
  DATE = {2014},
  ISBN = {978-1-315-83350-7},
  LANGID = {english},
  TITLE = {A Dictionary of Stylistics},
}

@ARTICLE{emotionregulation2,
  AUTHOR = {Walker, Matthew P. and van der Helm, Els},
  URL = {https://doi.org/10.1037/a0016570},
  DATE = {2009},
  DOI = {10.1037/a0016570},
  JOURNALTITLE = {Psychological Bulletin},
  NOTE = {Publisher: American Psychological Association ({APA})},
  NUMBER = {5},
  PAGES = {731--748},
  TITLE = {Overnight therapy? The role of sleep in emotional brain processing.},
  VOLUME = {135},
}

@ARTICLE{wangEmotionalIntelligenceLarge2023,
  ABSTRACT = {Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI. This test is an objective, performance-driven, and text-based evaluation, which requires evaluating complex emotions in realistic scenarios, providing a consistent assessment for both human and LLM capabilities. With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achieved above-average Emotional Quotient (EQ) scores, with GPT-4 exceeding 89\% of human participants with an EQ of 117. Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not rely on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/},
  AUTHOR = {Wang, Xuena and Li, Xueting and Yin, Zi and Wu, Yue and Liu, Jia},
  PUBLISHER = {SAGE Publications},
  DATE = {2023-01},
  DOI = {10.1177/18344909231213958},
  ISSN = {1834-4909},
  JOURNALTITLE = {Journal of Pacific Rim Psychology},
  LANGID = {english},
  PAGES = {18344909231213958},
  TITLE = {Emotional Intelligence of {{Large Language Models}}},
  VOLUME = {17},
}

@MISC{wangSelfConsistencyImprovesChain2023,
  ABSTRACT = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  AUTHOR = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  ORGANIZATION = {arXiv},
  DATE = {2023-03},
  DOI = {10.48550/arXiv.2203.11171},
  EPRINT = {2203.11171},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  TITLE = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
}

@MISC{wangUnleashingCognitiveSynergy2023,
  ABSTRACT = {Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assigning multiple, fine-grained personas in LLMs elicits better problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP effectively elicits internal knowledge acquisition abilities, reduces hallucination, and maintains strong reasoning capabilities. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.},
  AUTHOR = {Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng},
  ORGANIZATION = {arXiv},
  DATE = {2023-07},
  DOI = {10.48550/arXiv.2307.05300},
  EPRINT = {2307.05300},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  SHORTTITLE = {Unleashing {{Cognitive Synergy}} in {{Large Language Models}}},
  TITLE = {Unleashing {{Cognitive Synergy}} in {{Large Language Models}}: {{A Task-Solving Agent}} through {{Multi-Persona Self-Collaboration}}},
}

@ARTICLE{wardHierarchicalGroupingOptimize1963,
  AUTHOR = {Ward, Joe H.},
  DATE = {1963},
  ISSN = {0162-1459, 1537-274X},
  JOURNALTITLE = {Journal of the American Statistical Association},
  LANGID = {english},
  NUMBER = {301},
  PAGES = {236--244},
  TITLE = {Hierarchical Grouping to Optimize an Objective Function},
  VOLUME = {58},
}

@BOOK{watermanIntroductionComputationalBiology1995,
  ABSTRACT = {Biology is in the midst of a era yielding many significant discoveries and promising many more. Unique to this era is the exponential growth in the size of information-packed databases. Inspired by a pressing need to analyze that data, Introduction to Computational Biology explores a new area of expertise that emerged from this fertile field- the combination of biological and information sciences. This introduction describes the mathematical structure of biological data, especially from sequences and chromosomes. After a brief survey of molecular biology, it studies restriction maps of DNA, rough landmark maps of the underlying sequences, and clones and clone maps. It examines problems associated with reading DNA sequences and comparing sequences to finding common patterns. The author then considers that statistics of pattern counts in sequences, RNA secondary structure, and the inference of evolutionary history of related sequences.Introduction to Computational Biology exposes the reader to the fascinating structure of biological data and explains how to treat related combinatorial and statistical problems. Written to describe mathematical formulation and development, this book helps set the stage for even more, truly interdisciplinary work in biology.},
  AUTHOR = {Waterman, Michael S.},
  PUBLISHER = {CRC Press},
  DATE = {1995},
  ISBN = {978-0-412-99391-6},
  KEYWORDS = {Mathematics / Probability \textbackslash\& Statistics / General,Science / Biotechnology,Science / Life Sciences / Biology},
  LANGID = {english},
  SHORTTITLE = {Introduction to Computational Biology},
  TITLE = {Introduction to Computational Biology: {{Maps}}, Sequences and Genomes},
}

@MISC{weggeExperiencerSpecificEmotionAppraisal2023,
  ABSTRACT = {Emotion classification in NLP assigns emotions to texts, such as sentences or paragraphs. With texts like "I felt guilty when he cried", focusing on the sentence level disregards the standpoint of each participant in the situation: the writer ("I") and the other entity ("he") could in fact have different affective states. The emotions of different entities have been considered only partially in emotion semantic role labeling, a task that relates semantic roles to emotion cue words. Proposing a related task, we narrow the focus on the experiencers of events, and assign an emotion (if any holds) to each of them. To this end, we represent each emotion both categorically and with appraisal variables, as a psychological access to explaining why a person develops a particular emotion. On an event description corpus, our experiencer-aware models of emotions and appraisals outperform the experiencer-agnostic baselines, showing that disregarding event participants is an oversimplification for the emotion detection task.},
  AUTHOR = {Wegge, Maximilian and Troiano, Enrica and Oberländer, Laura and Klinger, Roman},
  ORGANIZATION = {arXiv},
  DATE = {2023-05},
  DOI = {10.48550/arXiv.2210.12078},
  EPRINT = {2210.12078},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Computation and Language},
  TITLE = {Experiencer-{{Specific Emotion}} and {{Appraisal Prediction}}},
}

@MISC{weiChainofThoughtPromptingElicits2023,
  ABSTRACT = {We explore how generating a chain of thought – a series of intermediate reasoning steps – significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  AUTHOR = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  ORGANIZATION = {arXiv},
  DATE = {2023-01},
  DOI = {10.48550/arXiv.2201.11903},
  EPRINT = {2201.11903},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  TITLE = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
}

@INPROCEEDINGS{wei-etal-2020-effective,
  AUTHOR = {Wei, Penghui and Zhao, Jiahao and Mao, Wenji},
  LOCATION = {Online},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2020.acl-main.289},
  BOOKTITLE = ACL:2020:MAIN,
  DATE = {2020-07},
  DOI = {10.18653/v1/2020.acl-main.289},
  PAGES = {3171--3181},
  TITLE = {Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction},
}

@ARTICLE{wharton_natural_2003,
  AUTHOR = {Wharton, Tim},
  DATE = {2003},
  FILE = {Full Text:/Users/bonard/switchdrive2/Zotero/storage/HVC7F7ST/Wharton - 2003 - Natural pragmatics and natural codes.pdf:application/pdf;Snapshot:/Users/bonard/switchdrive2/Zotero/storage/CJJ7BKV8/1468-0017.html:text/html},
  JOURNALTITLE = {Mind \& language},
  NOTE = {Publisher: Wiley Online Library},
  NUMBER = {5},
  PAGES = {447--477},
  TITLE = {Natural pragmatics and natural codes},
  VOLUME = {18},
}

@ARTICLE{wharton_that_2016,
  AUTHOR = {Wharton, Tim},
  DATE = {2016},
  FILE = {Snapshot:/Users/bonard/switchdrive2/Zotero/storage/GSX69I42/S0024384115001631.html:text/html},
  JOURNALTITLE = {Lingua},
  NOTE = {Publisher: Elsevier},
  PAGES = {20--35},
  SHORTTITLE = {That bloody so-and-so has retired},
  TITLE = {That bloody so-and-so has retired: {Expressives} revisited},
  VOLUME = {175},
}

@ARTICLE{wharton_relevance_2021,
  AUTHOR = {Wharton, Tim and Bonard, Constant and Dukes, Daniel and Sander, David and Oswald, Steve},
  LANGUAGE = {English},
  DATE = {2021},
  FILE = {Snapshot:/Users/bonard/switchdrive2/Zotero/storage/KY5RI4R2/relevance-and-emotion.html:text/html},
  JOURNALTITLE = {Journal of Pragmatics},
  PAGES = {259--269},
  TITLE = {Relevance and emotion},
  VOLUME = {181},
}

@ARTICLE{whiteReceivingSocialSupport2001,
  ABSTRACT = {Online support groups are expanding as the general public becomes more comfortable using computer-mediated communication technology. These support groups have certain benefits for users who may not be able to or do not have the desire to attend face-to-face sessions. Online support groups also present challenges when compared to traditional face-to-face group communication. Communication difficulties may arise resulting from lack of visual and aural cues found in traditional face-to-face communication. Online support groups have emerged within health care as a result of the need individuals have to know more about health conditions they are confronting. The proliferation of these online communities may provide an opportunity for health educators to reach target populations with specific messages. This paper reviews the development of health-related online support groups, examines research conducted within these communities, compares their utility with traditional support groups and discusses the implications of these groups for health education.},
  AUTHOR = {White, M. and Dorman, S. M.},
  DATE = {2001-12},
  DOI = {10.1093/her/16.6.693},
  EPRINT = {11780708},
  EPRINTTYPE = {pubmed},
  ISSN = {0268-1153},
  JOURNALTITLE = {Health Education Research},
  KEYWORDS = {Attitude to Computers,Computer Communication Networks,Disease,Health Education,Health Services Accessibility,Humans,Interpersonal Relations,Online Systems,Patient Participation,Self-Help Groups,Social Support,United States},
  LANGID = {english},
  NUMBER = {6},
  PAGES = {693--707},
  SHORTTITLE = {Receiving Social Support Online},
  TITLE = {Receiving Social Support Online: {{Implications}} for Health Education},
  VOLUME = {16},
}

@BOOK{whiteNarrativeMeansTherapeutic1990,
  ABSTRACT = {Use of letter-writing in family therapy.White and Epston base their therapy on the assumption that people experience problems when the stories of their lives, as they or others have invented them, do not sufficiently represent their lived experience. Therapy then becomes a process of storying or restorying the lives and experiences of these people. In this way narrative comes to play a central role in therapy. Both authors share delightful examples of a storied therapy that privileges a person’s lived experience, inviting a reflexive posture and encouraging a sense of authorship and reauthorship of one’s experiences and relationships in the telling and retelling of one’s story.},
  AUTHOR = {White, Michael and Epston, David},
  PUBLISHER = {WW Norton},
  DATE = {1990},
  ISBN = {978-0-393-70098-5},
  KEYWORDS = {Medical / Psychiatry / General,Psychology / General,Psychology / Psychopathology / Compulsive Behavior,Psychology / Psychotherapy / General},
  LANGID = {english},
  TITLE = {Narrative Means to Therapeutic Ends},
}

@INCOLLECTION{wilson_relevance_2006,
  AUTHOR = {Wilson, Deirdre and Sperber, Dan},
  EDITOR = {Horn, Laurence},
  LOCATION = {Oxford},
  PUBLISHER = {Blackwell},
  BOOKTITLE = {The {Handbook} of pragmatics},
  DATE = {2006},
  TITLE = {Relevance theory},
}

@INCOLLECTION{dreamstanford,
  AUTHOR = {Windt, Jennifer M.},
  EDITOR = {Zalta, Edward N.},
  PUBLISHER = {Metaphysics Research Lab, Stanford University},
  URL = {https://plato.stanford.edu/archives/sum2021/entries/dreams-dreaming/},
  BOOKTITLE = {The Stanford encyclopedia of philosophy},
  DATE = {2021},
  EDITION = {Summer 2021},
  TITLE = {Dreams and dreaming},
}

@BOOK{dreamscale,
  AUTHOR = {Winget, Carolyn and Kramer, Milton},
  PUBLISHER = {Gainesville: University of Florida Press},
  DATE = {1979},
  TITLE = {Dimensions of dreams},
}

@INPROCEEDINGS{wolfTransformersStateoftheArtNatural2020,
  ABSTRACT = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified {API}. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
  AUTHOR = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  EDITOR = {Liu, Qun and Schlangen, David},
  LOCATION = {Online},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2020.emnlp-demos.6/},
  BOOKTITLE = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  DATE = {2020-10},
  DOI = {10.18653/v1/2020.emnlp-demos.6},
  FILE = {Full Text PDF:C\:\\Users\\Gustave\\Zotero\\storage\\C3QH8HYD\\Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Processing.pdf:application/pdf},
  PAGES = {38--45},
  SHORTTITLE = {Transformers},
  TITLE = {Transformers: State-of-the-Art Natural Language Processing},
  URLDATE = {2025-09-24},
}

@ARTICLE{lamini-lm,
  AUTHOR = {Wu, Minghao and Waheed, Abdul and Zhang, Chiyu and Abdul-Mageed, Muhammad and Aji, Alham Fikri},
  DATE = {2023},
  JOURNALTITLE = {{CoRR}},
  TITLE = {{LaMini}-{LM}: a diverse herd of distilled models from large-scale instructions},
  VOLUME = {abs/2304.14402},
}

@INPROCEEDINGS{xia-ding-2019-emotion,
  AUTHOR = {Xia, Rui and Ding, Zixiang},
  LOCATION = {Florence, Italy},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/P19-1096},
  BOOKTITLE = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  DATE = {2019-07},
  DOI = {10.18653/v1/P19-1096},
  PAGES = {1003--1012},
  TITLE = {Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts},
}

@MISC{xuExpertPromptingInstructingLarge2023,
  ABSTRACT = {The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\textbackslash\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \textbackslash url\{https://github.com/OFA-Sys/ExpertLLaMA\}.},
  AUTHOR = {Xu, Benfeng and Yang, An and Lin, Junyang and Wang, Quan and Zhou, Chang and Zhang, Yongdong and Mao, Zhendong},
  ORGANIZATION = {arXiv},
  DATE = {2023-05},
  DOI = {10.48550/arXiv.2305.14688},
  EPRINT = {2305.14688},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  SHORTTITLE = {{{ExpertPrompting}}},
  TITLE = {{{ExpertPrompting}}: {{Instructing Large Language Models}} to Be {{Distinguished Experts}}},
}

@ARTICLE{yan2019process,
  AUTHOR = {Yan, Hengbin and Webster, Jonathan J.},
  DATE = {2014},
  DOI = {10.1080/21698252.2014.937563},
  JOURNALTITLE = {Journal of World Languages},
  NUMBER = {2},
  PAGES = {157--170},
  TITLE = {Automatic Labelling of Transitivity Functional Roles},
  VOLUME = {1},
}

@MISC{yaoReActSynergizingReasoning2023,
  ABSTRACT = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  AUTHOR = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  ORGANIZATION = {arXiv},
  DATE = {2023-03},
  DOI = {10.48550/arXiv.2210.03629},
  EPRINT = {2210.03629},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  SHORTTITLE = {{{ReAct}}},
  TITLE = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
}

@INPROCEEDINGS{yoranAnsweringQuestionsMetaReasoning2023,
  ABSTRACT = {Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregate their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, enabling humans to verify its answers.},
  AUTHOR = {Yoran, Ori and Wolfson, Tomer and Bogin, Ben and Katz, Uri and Deutch, Daniel and Berant, Jonathan},
  EDITOR = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  LOCATION = {Singapore},
  PUBLISHER = ACL,
  BOOKTITLE = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  DATE = {2023-12},
  DOI = {10.18653/v1/2023.emnlp-main.364},
  PAGES = {5942--5966},
  TITLE = {Answering {{Questions}} by {{Meta-Reasoning}} over {{Multiple Chains}} of {{Thought}}},
}

@ARTICLE{Yu2022,
  AUTHOR = {Yu, Calvin Kai-Ching},
  URL = {https://doi.org/10.1037/drm0000189},
  DATE = {2022-03},
  DOI = {10.1037/drm0000189},
  JOURNALTITLE = {Dreaming : journal of the Association for the Study of Dreams},
  NOTE = {Publisher: American Psychological Association ({APA})},
  NUMBER = {1},
  PAGES = {33--51},
  SHORTJOURNAL = {Dreaming},
  TITLE = {Automated analysis of dream sentiment—The royal road to dream dynamics?},
  VOLUME = {32},
}

@MISC{yuanSelfRewardingLanguageModels2024,
  ABSTRACT = {We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continually improve in both axes.},
  AUTHOR = {Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  ORGANIZATION = {arXiv},
  DATE = {2024-01},
  EPRINT = {2401.10020},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  TITLE = {Self-{{Rewarding Language Models}}},
}

@INPROCEEDINGS{zhanEvaluatingSubjectiveCognitive2023,
  ABSTRACT = {The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values (Scherer, 2005). Thus, the same situation can often result in different emotional experiences. While the detection of emotion is a well-established task, there is very limited work so far on the automatic prediction of cognitive appraisals. This work fills the gap by presenting CovidET-Appraisals, the most comprehensive dataset to-date that assesses 24 appraisal dimensions, each with a natural language rationale, across 241 Reddit posts. CovidET-Appraisals presents an ideal testbed to evaluate the ability of large language models — excelling at a wide range of NLP tasks — to automatically assess and explain cognitive appraisals. We found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models. We release our dataset at https://github.com/honglizhan/CovidET-Appraisals-Public.},
  AUTHOR = {Zhan, Hongli and Ong, Desmond and Li, Junyi Jessy},
  EDITOR = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  LOCATION = {Singapore},
  PUBLISHER = ACL,
  BOOKTITLE = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  DATE = {2023-12},
  DOI = {10.18653/v1/2023.findings-emnlp.962},
  PAGES = {14418--14446},
  TITLE = {Evaluating {{Subjective Cognitive Appraisals}} of {{Emotions}} from {{Large Language Models}}},
}

@INPROCEEDINGS{zhang-etal-2021-aspect-sentiment,
  AUTHOR = {Zhang, Wenxuan and Deng, Yang and Li, Xin and Yuan, Yifei and Bing, Lidong and Lam, Wai},
  LOCATION = {Online and Punta Cana, Dominican Republic},
  PUBLISHER = ACL,
  URL = {https://aclanthology.org/2021.emnlp-main.726},
  BOOKTITLE = {Proceedings of the 2021 conference on empirical methods in natural language processing},
  DATE = {2021-11},
  DOI = {10.18653/v1/2021.emnlp-main.726},
  PAGES = {9209--9219},
  TITLE = {Aspect sentiment quad prediction as paraphrase generation},
}

@UNPUBLISHED{zhangSurveyAspectBasedSentiment2022,
  ABSTRACT = {As an important fine-grained sentiment analysis problem, aspect-based sentiment analysis (ABSA), aiming to analyze and understand people's opinions at the aspect level, has been attracting considerable interest in the last decade. To handle ABSA in different scenarios, various tasks have been introduced for analyzing different sentiment elements and their relations, including the aspect term, aspect category, opinion term, and sentiment polarity. Unlike early ABSA works focusing on a single sentiment element, many compound ABSA tasks involving multiple elements have been studied in recent years for capturing more complete aspect-level sentiment information. However, a systematic review of various ABSA tasks and their corresponding solutions is still lacking, which we aim to fill in this survey. More specifically, we provide a new taxonomy for ABSA which organizes existing studies from the axes of concerned sentiment elements, with an emphasis on recent advances of compound ABSA tasks. From the perspective of solutions, we summarize the utilization of pre-trained language models for ABSA, which improved the performance of ABSA to a new stage. Besides, techniques for building more practical ABSA systems in cross-domain/lingual scenarios are discussed. Finally, we review some emerging topics and discuss some open challenges to outlook potential future directions of ABSA.},
  AUTHOR = {Zhang, Wenxuan and Li, Xin and Deng, Yang and Bing, Lidong and Lam, Wai},
  DATE = {2022-03},
  EPRINT = {2203.01054},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Computation and Language,notion},
  SHORTTITLE = {A {{Survey}} on {{Aspect-Based Sentiment Analysis}}},
  TITLE = {A {{Survey}} on {{Aspect-Based Sentiment Analysis}}: {{Tasks}}, {{Methods}}, and {{Challenges}}},
}

@MISC{zhangSentimentAnalysisEra2023,
  ABSTRACT = {Sentiment analysis (SA) has been a long-standing research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, \textbackslash textsc\{SentiEval\}, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at \textbackslash url\{https://github.com/DAMO-NLP-SG/LLM-Sentiment\}.},
  AUTHOR = {Zhang, Wenxuan and Deng, Yue and Liu, Bing and Pan, Sinno Jialin and Bing, Lidong},
  ORGANIZATION = {arXiv},
  DATE = {2023-05},
  DOI = {10.48550/arXiv.2305.15005},
  EPRINT = {2305.15005},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Computation and Language},
  SHORTTITLE = {Sentiment {{Analysis}} in the {{Era}} of {{Large Language Models}}},
  TITLE = {Sentiment {{Analysis}} in the {{Era}} of {{Large Language Models}}: {{A Reality Check}}},
}

@MISC{zhangIgnitingLanguageIntelligence2023,
  ABSTRACT = {Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics. A repository for the related papers is available at https://github.com/Zoeyyao27/CoT-Igniting-Agent.},
  AUTHOR = {Zhang, Zhuosheng and Yao, Yao and Zhang, Aston and Tang, Xiangru and Ma, Xinbei and He, Zhiwei and Wang, Yiming and Gerstein, Mark and Wang, Rui and Liu, Gongshen and Zhao, Hai},
  ORGANIZATION = {arXiv},
  DATE = {2023-11},
  DOI = {10.48550/arXiv.2311.11797},
  EPRINT = {2311.11797},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Multiagent Systems},
  SHORTTITLE = {Igniting {{Language Intelligence}}},
  TITLE = {Igniting {{Language Intelligence}}: {{The Hitchhiker}}'s {{Guide From Chain-of-Thought Reasoning}} to {{Language Agents}}},
}

@ARTICLE{Zheng2021,
  AUTHOR = {Zheng, Xiaofang and Schweickert, Richard},
  URL = {https://doi.org/10.1037/drm0000173},
  DATE = {2021-09},
  DOI = {10.1037/drm0000173},
  JOURNALTITLE = {Dreaming : journal of the Association for the Study of Dreams},
  NOTE = {Publisher: American Psychological Association ({APA})},
  NUMBER = {3},
  PAGES = {207--224},
  SHORTJOURNAL = {Dreaming},
  TITLE = {Comparing hall Van de Castle coding and Linguistic Inquiry and Word Count using canonical correlation analysis.},
  VOLUME = {31},
}

@MISC{zhouHowFaRAre2023,
  ABSTRACT = {"Thinking is for Doing." Humans can infer other people's mental states from observations–an ability called Theory-of-Mind (ToM)–and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4's performance from 50\% to 71\% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning.},
  AUTHOR = {Zhou, Pei and Madaan, Aman and Potharaju, Srividya Pranavi and Gupta, Aditya and McKee, Kevin R. and Holtzman, Ari and Pujara, Jay and Ren, Xiang and Mishra, Swaroop and Nematzadeh, Aida and Upadhyay, Shyam and Faruqui, Manaal},
  ORGANIZATION = {arXiv},
  DATE = {2023-10},
  DOI = {10.48550/arXiv.2310.03051},
  EPRINT = {2310.03051},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  TITLE = {How {{FaR Are Large Language Models From Agents}} with {{Theory-of-Mind}}?},
}

@MISC{zhouSelfDiscoverLargeLanguage2024,
  ABSTRACT = {We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32\% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20\%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.},
  AUTHOR = {Zhou, Pei and Pujara, Jay and Ren, Xiang and Chen, Xinyun and Cheng, Heng-Tze and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Mishra, Swaroop and Zheng, Huaixiu Steven},
  ORGANIZATION = {arXiv},
  DATE = {2024-02},
  EPRINT = {2402.03620},
  EPRINTCLASS = {cs},
  EPRINTTYPE = {arXiv},
  KEYWORDS = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  SHORTTITLE = {Self-{{Discover}}},
  TITLE = {Self-{{Discover}}: {{Large Language Models Self-Compose Reasoning Structures}}},
}

@INPROCEEDINGS{ziemsInducingPositivePerspectives2022,
  ABSTRACT = {Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning. Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task. To facilitate rapid progress, we introduce a large-scale benchmark, Positive Psychology Frames, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically-motivated reframing strategies. Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work.},
  AUTHOR = {Ziems, Caleb and Li, Minzhi and Zhang, Anthony and Yang, Diyi},
  EDITOR = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  LOCATION = {Dublin, Ireland},
  PUBLISHER = ACL,
  BOOKTITLE = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  DATE = {2022-05},
  DOI = {10.18653/v1/2022.acl-long.257},
  PAGES = {3682--3700},
  TITLE = {Inducing {{Positive Perspectives}} with {{Text Reframing}}},
}

@INPROCEEDINGS{ziemsInducingPositivePerspectives2022a,
  ABSTRACT = {Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning. Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task. To facilitate rapid progress, we introduce a large-scale benchmark, Positive Psychology Frames, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically-motivated reframing strategies. Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work.},
  AUTHOR = {Ziems, Caleb and Li, Minzhi and Zhang, Anthony and Yang, Diyi},
  EDITOR = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  LOCATION = {Dublin, Ireland},
  PUBLISHER = ACL,
  BOOKTITLE = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  DATE = {2022},
  FILE = {C:\Users\Gustave\Zotero\storage\PJD9NQY9\Ziems et al. - 2022 - Inducing Positive Perspectives with Text Reframing.pdf},
  PAGES = {3682--3700},
  TITLE = {Inducing Positive Perspectives with Text Reframing},
}

@ARTICLE{Zlotowicz73,
  AUTHOR = {Zlotowicz, Michel},
  URL = {https://www.persee.fr/doc/bupsy_0007-4403_1973_num_26_305_10419},
  DATE = {1973},
  JOURNALTITLE = {Bulletin de psychologie},
  NUMBER = {305},
  PAGES = {615--621},
  TITLE = {Sur l'analyse du cauchemar enfantin},
  VOLUME = {26},
}

@ARTICLE{zwaanSituationModelsLanguage1998,
  ABSTRACT = {This article reviews research on the use of situation models in language comprehension and memory retrieval over the past 15 years. Situation models are integrated mental representations of a described state of affairs. Significant progress has been made in the scientific understanding of how situation models are involved in language comprehension and memory retrieval. Much of this research focuses on establishing the existence of situation models, often by using tasks that assess one dimension of a situation model. However, the authors argue that the time has now come for researchers to begin to take the multidimensionality of situation models seriously. The authors offer a theoretical framework and some methodological observations that may help researchers to tackle this issue.},
  AUTHOR = {Zwaan, R. A. and Radvansky, G. A.},
  DATE = {1998-03},
  DOI = {10.1037/0033-2909.123.2.162},
  EPRINT = {9522683},
  EPRINTTYPE = {pubmed},
  ISSN = {0033-2909},
  JOURNALTITLE = {Psychological Bulletin},
  KEYWORDS = {Humans,Logic,Memory,Models,Psychological,Reading},
  LANGID = {english},
  NUMBER = {2},
  PAGES = {162--185},
  TITLE = {Situation Models in Language Comprehension and Memory},
  VOLUME = {123},
}

